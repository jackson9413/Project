{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01a1b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6ffb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03c427ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file\n",
    "data = pd.read_csv('Students.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c17f3fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (3614, 16)\n"
     ]
    }
   ],
   "source": [
    "# data shape\n",
    "print(f\"Data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06821bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUPLICATE ANALYSIS ===\n",
      "Original data duplicates (including all columns): 0\n",
      "Dropped identifying columns: ['Student_Name', 'State', 'College_Name', 'Stream']\n",
      "Duplicates after dropping identifying columns: 580\n",
      "✅ 580 students have identical responses across all features\n",
      "   This is normal - multiple students can have the same usage patterns\n",
      "\n",
      "📊 Sample of identical response patterns:\n",
      "Showing 6 rows with identical patterns...\n",
      "    Year_of_Study  Daily_Usage_Hours  Trust_in_AI_Tools  Impact_on_Grades\n",
      "0               4                0.9                  2                 2\n",
      "4               1                0.9                  1                 3\n",
      "9               2                1.4                  3                -2\n",
      "19              1                4.4                  1                 2\n",
      "22              1                3.4                  1                 0\n",
      "32              2                0.7                  3                 0\n"
     ]
    }
   ],
   "source": [
    "# Check duplicates in original data\n",
    "print(\"=== DUPLICATE ANALYSIS ===\")\n",
    "duplicates_original = data.duplicated().sum()\n",
    "print(f\"Original data duplicates (including all columns): {duplicates_original}\")\n",
    "\n",
    "# Check duplicates after dropping identifying columns\n",
    "identifying_cols = ['Student_Name', 'State', 'College_Name', 'Stream']\n",
    "existing_identifying_cols = [col for col in identifying_cols if col in data.columns]\n",
    "data_without_ids = data.drop(columns=existing_identifying_cols)\n",
    "duplicates_after_drop = data_without_ids.duplicated().sum()\n",
    "\n",
    "print(f\"Dropped identifying columns: {existing_identifying_cols}\")\n",
    "print(f\"Duplicates after dropping identifying columns: {duplicates_after_drop}\")\n",
    "\n",
    "if duplicates_after_drop > 0:\n",
    "    print(f\"✅ {duplicates_after_drop} students have identical responses across all features\")\n",
    "    print(\"   This is normal - multiple students can have the same usage patterns\")\n",
    "else:\n",
    "    print(\"✅ No duplicate response patterns found\")\n",
    "\n",
    "# Optional: Show some examples of identical response patterns\n",
    "if duplicates_after_drop > 0:\n",
    "    print(\"\\n📊 Sample of identical response patterns:\")\n",
    "    duplicate_mask = data_without_ids.duplicated(keep=False)\n",
    "    sample_duplicates = data_without_ids[duplicate_mask].head(6)\n",
    "    print(f\"Showing {len(sample_duplicates)} rows with identical patterns...\")\n",
    "    print(sample_duplicates[['Year_of_Study', 'Daily_Usage_Hours', 'Trust_in_AI_Tools', 'Impact_on_Grades']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d98e94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identifying columns while keeping identical response patterns\n",
    "# Note: We keep rows with identical responses because they represent different students\n",
    "# with the same usage patterns, which is valuable data for modeling\n",
    "data_clean = data.drop(columns=['State', 'Student_Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c94020",
   "metadata": {},
   "source": [
    "# Use Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4ff21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column for Use_Cases split by a comma \n",
    "data_clean['Use_Cases_Split'] = data_clean['Use_Cases'].str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d51e024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All individual use cases and their frequencies:\n",
      "'coding help': 766\n",
      "'assignments': 749\n",
      "'mcq practice': 692\n",
      "'doubt solving': 638\n",
      "'resume writing': 617\n",
      "'content writing': 602\n",
      "'learning new topics': 594\n",
      "'projects': 470\n",
      "'exam prep': 451\n",
      "'notes': 451\n",
      "'project work': 229\n",
      "'exam preparation': 202\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# First, let's extract all individual use cases from the split data\n",
    "def extract_all_individual_use_cases(df, column='Use_Cases_Split'):\n",
    "    \"\"\"\n",
    "    Extract all individual use cases from the split column\n",
    "    \"\"\"\n",
    "    all_individual_cases = []\n",
    "    for case_list in df[column].dropna():\n",
    "        for case in case_list:\n",
    "            if case.strip():  # Only add non-empty cases\n",
    "                all_individual_cases.append(case.strip().lower())\n",
    "    return all_individual_cases\n",
    "\n",
    "# Get all individual use cases\n",
    "all_cases = extract_all_individual_use_cases(data_clean)\n",
    "case_counts = Counter(all_cases)\n",
    "\n",
    "print(\"All individual use cases and their frequencies:\")\n",
    "for case, count in case_counts.most_common():\n",
    "    print(f\"'{case}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3eccd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_individual_use_cases(df):\n",
    "    \"\"\"\n",
    "    Standardize individual use cases using the mapping\n",
    "    \"\"\"\n",
    "    def apply_mapping_to_list(case_list):\n",
    "        # Handle None/NaN cases\n",
    "        if case_list is None or (isinstance(case_list, float) and pd.isna(case_list)):\n",
    "            return case_list\n",
    "        \n",
    "        # Handle empty list\n",
    "        if not isinstance(case_list, list) or len(case_list) == 0:\n",
    "            return case_list\n",
    "        \n",
    "        standardized_cases = []\n",
    "        for case in case_list:\n",
    "            if case and isinstance(case, str):  # Check if case is not None and is string\n",
    "                # Clean and normalize the case\n",
    "                cleaned_case = case.strip().lower()\n",
    "                # Apply mapping\n",
    "                standardized_case = use_case_mapping.get(cleaned_case, cleaned_case)\n",
    "                # Avoid duplicates\n",
    "                if standardized_case not in standardized_cases:\n",
    "                    standardized_cases.append(standardized_case)\n",
    "        \n",
    "        return standardized_cases\n",
    "    \n",
    "    return df['Use_Cases_Split'].apply(apply_mapping_to_list)\n",
    "\n",
    "# Apply standardization\n",
    "data_clean['Use_Cases_Split_Standardized'] = standardize_individual_use_cases(data_clean)\n",
    "\n",
    "# Convert back to string format for easier viewing\n",
    "data_clean['Use_Cases_Standardized'] = data_clean['Use_Cases_Split_Standardized'].apply(\n",
    "    lambda x: ', '.join([case.title() for case in x]) if x and isinstance(x, list) else ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "541d88e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary use case features:\n",
      "uses_coding_help: 766 students\n",
      "uses_assignments: 749 students\n",
      "uses_project_work: 699 students\n",
      "uses_mcq_practice: 692 students\n",
      "uses_exam_preparation: 653 students\n",
      "uses_doubt_solving: 638 students\n",
      "uses_resume_writing: 617 students\n",
      "uses_content_writing: 602 students\n",
      "uses_learning_new_topics: 594 students\n",
      "uses_notes: 451 students\n"
     ]
    }
   ],
   "source": [
    "# Create binary features for each use case\n",
    "def create_binary_use_case_features(df):\n",
    "    \"\"\"\n",
    "    Create binary columns for each unique use case\n",
    "    \"\"\"\n",
    "    # Get all unique use cases from standardized data\n",
    "    all_use_cases = ['coding help', 'assignments', 'project work', 'mcq practice', \n",
    "                     'exam preparation', 'doubt solving', 'resume writing', \n",
    "                     'content writing', 'learning new topics', 'notes']\n",
    "    \n",
    "    # Create binary columns\n",
    "    for use_case in all_use_cases:\n",
    "        col_name = f'uses_{use_case.replace(\" \", \"_\")}'\n",
    "        df[col_name] = df['Use_Cases_Split_Standardized'].apply(\n",
    "            lambda x: 1 if x and use_case in x else 0\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function\n",
    "data_clean = create_binary_use_case_features(data_clean)\n",
    "\n",
    "# Show the new binary features\n",
    "binary_features = [col for col in data_clean.columns if col.startswith('uses_')]\n",
    "print(\"Binary use case features:\")\n",
    "for feature in binary_features:\n",
    "    print(f\"{feature}: {data_clean[feature].sum()} students\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f9b729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5\n",
    "# data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82339c",
   "metadata": {},
   "source": [
    "# AI_Tools_Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d851624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING DUMMY VARIABLES FOR AI TOOLS ===\n",
      "Method 1: Using pandas get_dummies\n",
      "Created 7 dummy variables:\n",
      "  • ai_tool_bard: 151 students use this tool\n",
      "  • ai_tool_chatgpt: 1557 students use this tool\n",
      "  • ai_tool_claude: 171 students use this tool\n",
      "  • ai_tool_copilot: 1516 students use this tool\n",
      "  • ai_tool_gemini: 1409 students use this tool\n",
      "  • ai_tool_midjourney: 371 students use this tool\n",
      "  • ai_tool_other: 167 students use this tool\n",
      "\n",
      "Dummy variables shape: (3614, 7)\n",
      "Sample of dummy variables:\n",
      "   ai_tool_bard  ai_tool_chatgpt  ai_tool_claude  ai_tool_copilot  \\\n",
      "0             0                0               0                0   \n",
      "1             0                1               0                0   \n",
      "2             0                0               0                1   \n",
      "3             0                0               0                1   \n",
      "4             0                0               0                0   \n",
      "\n",
      "   ai_tool_gemini  ai_tool_midjourney  ai_tool_other  \n",
      "0               1                   0              0  \n",
      "1               0                   0              0  \n",
      "2               0                   0              0  \n",
      "3               0                   0              0  \n",
      "4               1                   0              0  \n",
      "\n",
      "Data types:\n",
      "All columns are integers: True\n"
     ]
    }
   ],
   "source": [
    "def create_ai_tools_dummy_variables(df, column_name='AI_Tools_Used'):\n",
    "    \"\"\"\n",
    "    Create dummy variables for comma-separated AI tools column\n",
    "    \"\"\"\n",
    "    print(\"=== CREATING DUMMY VARIABLES FOR AI TOOLS ===\")\n",
    "    \n",
    "    # Method 1: Using pandas get_dummies with separator\n",
    "    print(\"Method 1: Using pandas get_dummies\")\n",
    "    \n",
    "    # Create dummy variables directly from comma-separated values\n",
    "    # Convert to int to get 1/0 instead of True/False\n",
    "    dummies = df[column_name].str.get_dummies(sep=', ').astype(int)\n",
    "    \n",
    "    # Add prefix to column names for clarity\n",
    "    dummies.columns = [f'ai_tool_{col.lower().replace(\" \", \"_\")}' for col in dummies.columns]\n",
    "    \n",
    "    print(f\"Created {len(dummies.columns)} dummy variables:\")\n",
    "    for col in dummies.columns:\n",
    "        print(f\"  • {col}: {dummies[col].sum()} students use this tool\")\n",
    "    \n",
    "    # Show the dummy variables\n",
    "    print(f\"\\nDummy variables shape: {dummies.shape}\")\n",
    "    print(f\"Sample of dummy variables:\")\n",
    "    print(dummies.head())\n",
    "    \n",
    "    # Verify data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(f\"All columns are integers: {all(dummies[col].dtype in ['int64', 'int32'] for col in dummies.columns)}\")\n",
    "    \n",
    "    return dummies\n",
    "\n",
    "# Apply the AI Tools dummy variables function to data_clean\n",
    "ai_tools_dummies_clean = create_ai_tools_dummy_variables(data_clean, 'AI_Tools_Used') \n",
    "# Apply to your data\n",
    "data_clean_v1 = pd.concat([data_clean, ai_tools_dummies_clean], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e919ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 of data clean v1\n",
    "# data_clean_v1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42f342",
   "metadata": {},
   "source": [
    "# Drop Necessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fffafb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['College_Name', 'Stream', 'Year_of_Study', 'AI_Tools_Used',\n",
       "       'Daily_Usage_Hours', 'Use_Cases', 'Trust_in_AI_Tools',\n",
       "       'Impact_on_Grades', 'Do_Professors_Allow_Use', 'Preferred_AI_Tool',\n",
       "       'Awareness_Level', 'Willing_to_Pay_for_Access', 'Device_Used',\n",
       "       'Internet_Access', 'Use_Cases_Split', 'Use_Cases_Split_Standardized',\n",
       "       'Use_Cases_Standardized', 'uses_coding_help', 'uses_assignments',\n",
       "       'uses_project_work', 'uses_mcq_practice', 'uses_exam_preparation',\n",
       "       'uses_doubt_solving', 'uses_resume_writing', 'uses_content_writing',\n",
       "       'uses_learning_new_topics', 'uses_notes', 'ai_tool_bard',\n",
       "       'ai_tool_chatgpt', 'ai_tool_claude', 'ai_tool_copilot',\n",
       "       'ai_tool_gemini', 'ai_tool_midjourney', 'ai_tool_other'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names of data clean \n",
    "data_clean_v1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9e4c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'Student_Name', 'College_Name', 'Use_Cases', 'State', 'Use_Cases_Advanced', 'Use_Cases_Split','Use_Cases_Length', 'Use_Cases_Split_Standardized'\n",
    "data_cleaned_final = data_clean_v1.drop(columns=['College_Name', 'Stream', 'AI_Tools_Used', 'Use_Cases', 'Use_Cases_Split', 'Use_Cases_Split_Standardized', 'Use_Cases_Standardized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ceb16ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 of data cleaned final\n",
    "# data_cleaned_final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ae383",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6c9ac2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded Preferred_AI_Tool: 6 features created\n",
      "One-hot encoded Device_Used: 3 features created\n",
      "One-hot encoded Internet_Access: 3 features created\n",
      "Binary encoded Do_Professors_Allow_Use: 0/1\n",
      "Binary encoded Willing_to_Pay_for_Access: 0/1\n",
      "\n",
      "Skipping Trust_in_AI_Tools and Impact_on_Grades - already numerical\n",
      "Trust_in_AI_Tools type: int64\n",
      "Impact_on_Grades type: int64\n",
      "\n",
      "New encoded columns created:\n",
      "['preferred_ai_tool_Bard', 'preferred_ai_tool_ChatGPT', 'preferred_ai_tool_Claude', 'preferred_ai_tool_Copilot', 'preferred_ai_tool_Gemini', 'preferred_ai_tool_Other', 'device_used_Laptop', 'device_used_Mobile', 'device_used_Tablet', 'internet_access_High', 'internet_access_Medium', 'internet_access_Poor', 'Do_Professors_Allow_Use_encoded', 'Willing_to_Pay_for_Access_encoded']\n",
      "\n",
      "Sample of encoded data:\n",
      "   Trust_in_AI_Tools  Impact_on_Grades  preferred_ai_tool_Bard  \\\n",
      "0                  2                 2                       0   \n",
      "1                  3                -3                       0   \n",
      "2                  5                 0                       0   \n",
      "3                  5                 2                       0   \n",
      "4                  1                 3                       0   \n",
      "\n",
      "   preferred_ai_tool_ChatGPT  preferred_ai_tool_Claude  \\\n",
      "0                          0                         0   \n",
      "1                          0                         0   \n",
      "2                          0                         0   \n",
      "3                          0                         0   \n",
      "4                          0                         0   \n",
      "\n",
      "   preferred_ai_tool_Copilot  preferred_ai_tool_Gemini  \n",
      "0                          1                         0  \n",
      "1                          0                         0  \n",
      "2                          0                         1  \n",
      "3                          0                         1  \n",
      "4                          0                         0  \n",
      "\n",
      "Data types of encoded columns:\n",
      "preferred_ai_tool_Bard: int32 | Unique values: [0, 1]\n",
      "preferred_ai_tool_ChatGPT: int32 | Unique values: [0, 1]\n",
      "preferred_ai_tool_Claude: int32 | Unique values: [0, 1]\n",
      "preferred_ai_tool_Copilot: int32 | Unique values: [0, 1]\n",
      "preferred_ai_tool_Gemini: int32 | Unique values: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "def apply_appropriate_encoding(df):\n",
    "    \"\"\"\n",
    "    Apply the correct encoding strategy for each categorical column\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. ONE-HOT ENCODING for nominal categories\n",
    "    nominal_columns = ['AI_Tools_Used', 'Preferred_AI_Tool', 'Device_Used', 'Internet_Access']\n",
    "    \n",
    "    for col in nominal_columns:\n",
    "        if col in df.columns:\n",
    "            # Create dummy variables with dtype=int to get 1/0 instead of True/False\n",
    "            dummies = pd.get_dummies(df[col], prefix=col.lower(), drop_first=False, dtype=int)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "            print(f\"One-hot encoded {col}: {len(dummies.columns)} features created\")\n",
    "    \n",
    "    # 2. BINARY ENCODING for yes/no columns only\n",
    "    binary_mappings = {\n",
    "        'Do_Professors_Allow_Use': {'No': 0, 'Yes': 1},\n",
    "        'Willing_to_Pay_for_Access': {'No': 0, 'Yes': 1}\n",
    "    }\n",
    "    \n",
    "    for col, mapping in binary_mappings.items():\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_encoded'] = df[col].map(mapping)\n",
    "            print(f\"Binary encoded {col}: 0/1\")\n",
    "    \n",
    "    # 3. Skip Trust_in_AI_Tools and Impact_on_Grades if they're already numerical\n",
    "    print(f\"\\nSkipping Trust_in_AI_Tools and Impact_on_Grades - already numerical\")\n",
    "    print(f\"Trust_in_AI_Tools type: {df['Trust_in_AI_Tools'].dtype}\")\n",
    "    print(f\"Impact_on_Grades type: {df['Impact_on_Grades'].dtype}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply encoding strategy\n",
    "data_encoded = apply_appropriate_encoding(data_cleaned_final)\n",
    "\n",
    "# Show the results\n",
    "print(\"\\nNew encoded columns created:\")\n",
    "encoded_cols = [col for col in data_encoded.columns if any(x in col for x in ['_encoded', 'stream_', 'ai_tools_used_', 'preferred_ai_tool_', 'device_used_', 'internet_access_'])]\n",
    "print(encoded_cols)\n",
    "\n",
    "# Display sample of encoded data\n",
    "print(\"\\nSample of encoded data:\")\n",
    "print(data_encoded[['Trust_in_AI_Tools', 'Impact_on_Grades'] + encoded_cols[:5]].head())\n",
    "\n",
    "# Verify data types of encoded columns\n",
    "print(\"\\nData types of encoded columns:\")\n",
    "for col in encoded_cols[:5]:\n",
    "    if col in data_encoded.columns:\n",
    "        print(f\"{col}: {data_encoded[col].dtype} | Unique values: {sorted(data_encoded[col].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e5591",
   "metadata": {},
   "source": [
    "# Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d77ca2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "data_encoded_clean = data_encoded.drop(columns=[\n",
    "    'Do_Professors_Allow_Use', 'Preferred_AI_Tool',\n",
    "    'Willing_to_Pay_for_Access', 'Device_Used',\n",
    "       'Internet_Access'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "09638745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final data shape after encoding: (3614, 36)\n"
     ]
    }
   ],
   "source": [
    "# data shape of encoded clean data\n",
    "print(f\"\\nFinal data shape after encoding: {data_encoded_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9150cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as a csv file\n",
    "data_encoded_clean.to_csv('Students_Cleaned_Encoded_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9cb2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 of data encoded\n",
    "# data_encoded.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a0dc6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8fd7740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Students_Cleaned_Encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ec0cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Study</th>\n",
       "      <th>Daily_Usage_Hours</th>\n",
       "      <th>Trust_in_AI_Tools</th>\n",
       "      <th>Impact_on_Grades</th>\n",
       "      <th>Awareness_Level</th>\n",
       "      <th>uses_coding_help</th>\n",
       "      <th>uses_assignments</th>\n",
       "      <th>uses_project_work</th>\n",
       "      <th>uses_mcq_practice</th>\n",
       "      <th>uses_exam_preparation</th>\n",
       "      <th>...</th>\n",
       "      <th>preferred_ai_tool_Gemini</th>\n",
       "      <th>preferred_ai_tool_Other</th>\n",
       "      <th>device_used_Laptop</th>\n",
       "      <th>device_used_Mobile</th>\n",
       "      <th>device_used_Tablet</th>\n",
       "      <th>internet_access_High</th>\n",
       "      <th>internet_access_Medium</th>\n",
       "      <th>internet_access_Poor</th>\n",
       "      <th>Do_Professors_Allow_Use_encoded</th>\n",
       "      <th>Willing_to_Pay_for_Access_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Study  Daily_Usage_Hours  Trust_in_AI_Tools  Impact_on_Grades  \\\n",
       "0              4                0.9                  2                 2   \n",
       "1              2                3.4                  3                -3   \n",
       "2              2                3.6                  5                 0   \n",
       "3              2                2.9                  5                 2   \n",
       "4              1                0.9                  1                 3   \n",
       "\n",
       "   Awareness_Level  uses_coding_help  uses_assignments  uses_project_work  \\\n",
       "0                9                 1                 1                  0   \n",
       "1                6                 0                 0                  0   \n",
       "2                1                 0                 0                  1   \n",
       "3                5                 0                 0                  0   \n",
       "4                8                 0                 0                  0   \n",
       "\n",
       "   uses_mcq_practice  uses_exam_preparation  ...  preferred_ai_tool_Gemini  \\\n",
       "0                  0                      0  ...                         0   \n",
       "1                  0                      0  ...                         0   \n",
       "2                  1                      0  ...                         1   \n",
       "3                  0                      0  ...                         1   \n",
       "4                  0                      0  ...                         0   \n",
       "\n",
       "   preferred_ai_tool_Other  device_used_Laptop  device_used_Mobile  \\\n",
       "0                        0                   0                   1   \n",
       "1                        1                   1                   0   \n",
       "2                        0                   0                   0   \n",
       "3                        0                   1                   0   \n",
       "4                        1                   1                   0   \n",
       "\n",
       "   device_used_Tablet  internet_access_High  internet_access_Medium  \\\n",
       "0                   0                     0                       0   \n",
       "1                   0                     0                       0   \n",
       "2                   1                     0                       0   \n",
       "3                   0                     1                       0   \n",
       "4                   0                     0                       1   \n",
       "\n",
       "   internet_access_Poor  Do_Professors_Allow_Use_encoded  \\\n",
       "0                     1                                0   \n",
       "1                     1                                1   \n",
       "2                     1                                0   \n",
       "3                     0                                1   \n",
       "4                     0                                1   \n",
       "\n",
       "   Willing_to_Pay_for_Access_encoded  \n",
       "0                                  1  \n",
       "1                                  0  \n",
       "2                                  0  \n",
       "3                                  0  \n",
       "4                                  1  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 5\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bea634c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE REDUNDANCY ANALYSIS ===\n",
      "AI Tool Usage Features: 7\n",
      "Preferred Tool Features: 6\n",
      "ChatGPT usage vs preference correlation: 0.020\n",
      "\n",
      "Highly correlated features (>0.8):\n",
      "  uses_exam_preparation ↔ uses_notes: 0.804\n"
     ]
    }
   ],
   "source": [
    "# 1. Check for redundant/duplicate features\n",
    "def analyze_feature_redundancy(data):\n",
    "    \"\"\"\n",
    "    Identify potentially redundant features\n",
    "    \"\"\"\n",
    "    print(\"=== FEATURE REDUNDANCY ANALYSIS ===\")\n",
    "    \n",
    "    # Check for duplicate AI tool features\n",
    "    ai_tool_features = [col for col in data.columns if col.startswith('ai_tool_')]\n",
    "    preferred_tool_features = [col for col in data.columns if col.startswith('preferred_ai_tool_')]\n",
    "    \n",
    "    print(f\"AI Tool Usage Features: {len(ai_tool_features)}\")\n",
    "    print(f\"Preferred Tool Features: {len(preferred_tool_features)}\")\n",
    "    \n",
    "    # Check correlation between similar features\n",
    "    if 'ai_tool_chatgpt' in data.columns and 'preferred_ai_tool_ChatGPT' in data.columns:\n",
    "        corr = data['ai_tool_chatgpt'].corr(data['preferred_ai_tool_ChatGPT'])\n",
    "        print(f\"ChatGPT usage vs preference correlation: {corr:.3f}\")\n",
    "    \n",
    "    # Look for highly correlated features\n",
    "    corr_matrix = data.corr()\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "                high_corr_pairs.append((\n",
    "                    corr_matrix.columns[i], \n",
    "                    corr_matrix.columns[j], \n",
    "                    corr_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    print(f\"\\nHighly correlated features (>0.8):\")\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"  {feat1} ↔ {feat2}: {corr:.3f}\")\n",
    "    \n",
    "    return high_corr_pairs\n",
    "\n",
    "# Run redundancy analysis\n",
    "redundant_features = analyze_feature_redundancy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40d238ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADVANCED FEATURE ENGINEERING ===\n",
      "Created 9 new features\n",
      "New features: ['ai_tool_diversity', 'use_case_diversity', 'tech_adoption_score', 'academic_focus', 'professional_focus', 'device_quality', 'internet_quality', 'trust_x_usage', 'year_x_usage']\n"
     ]
    }
   ],
   "source": [
    "# 2. Create more meaningful feature engineering\n",
    "def create_advanced_features(data):\n",
    "    \"\"\"\n",
    "    Create more sophisticated features\n",
    "    \"\"\"\n",
    "    print(\"=== ADVANCED FEATURE ENGINEERING ===\")\n",
    "    \n",
    "    data_enhanced = data.copy()\n",
    "    \n",
    "    # 1. AI Tool Diversity Score\n",
    "    ai_tool_cols = [col for col in data.columns if col.startswith('ai_tool_')]\n",
    "    data_enhanced['ai_tool_diversity'] = data[ai_tool_cols].sum(axis=1)\n",
    "    \n",
    "    # 2. Use Case Diversity Score\n",
    "    use_case_cols = [col for col in data.columns if col.startswith('uses_')]\n",
    "    data_enhanced['use_case_diversity'] = data[use_case_cols].sum(axis=1)\n",
    "    \n",
    "    # 3. Technology Adoption Score (composite)\n",
    "    tech_features = ['Trust_in_AI_Tools', 'Awareness_Level', 'ai_tool_diversity']\n",
    "    data_enhanced['tech_adoption_score'] = data_enhanced[tech_features].sum(axis=1)\n",
    "    \n",
    "    # 4. Academic Focus Score\n",
    "    academic_uses = ['uses_assignments', 'uses_exam_preparation', 'uses_project_work']\n",
    "    data_enhanced['academic_focus'] = data_enhanced[academic_uses].sum(axis=1)\n",
    "    \n",
    "    # 5. Professional Focus Score\n",
    "    professional_uses = ['uses_resume_writing', 'uses_content_writing', 'uses_coding_help']\n",
    "    data_enhanced['professional_focus'] = data_enhanced[professional_uses].sum(axis=1)\n",
    "    \n",
    "    # 6. Device Quality Score\n",
    "    if 'device_used_Laptop' in data.columns:\n",
    "        data_enhanced['device_quality'] = (\n",
    "            data_enhanced['device_used_Laptop'] * 3 +\n",
    "            data_enhanced['device_used_Tablet'] * 2 +\n",
    "            data_enhanced['device_used_Mobile'] * 1\n",
    "        )\n",
    "    \n",
    "    # 7. Internet Quality Score\n",
    "    if 'internet_access_High' in data.columns:\n",
    "        data_enhanced['internet_quality'] = (\n",
    "            data_enhanced['internet_access_High'] * 3 +\n",
    "            data_enhanced['internet_access_Medium'] * 2 +\n",
    "            data_enhanced['internet_access_Poor'] * 1\n",
    "        )\n",
    "    \n",
    "    # 8. Interaction terms\n",
    "    data_enhanced['trust_x_usage'] = data_enhanced['Trust_in_AI_Tools'] * data_enhanced['Daily_Usage_Hours']\n",
    "    data_enhanced['year_x_usage'] = data_enhanced['Year_of_Study'] * data_enhanced['Daily_Usage_Hours']\n",
    "    \n",
    "    print(f\"Created {len(data_enhanced.columns) - len(data.columns)} new features\")\n",
    "    \n",
    "    new_features = [col for col in data_enhanced.columns if col not in data.columns]\n",
    "    print(f\"New features: {new_features}\")\n",
    "    \n",
    "    return data_enhanced, new_features\n",
    "\n",
    "# Apply advanced feature engineering\n",
    "data_enhanced, new_features = create_advanced_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a65f4c",
   "metadata": {},
   "source": [
    "## 📊 Analysis of create_advanced_features Function\n",
    "\n",
    "### ✅ **STRENGTHS:**\n",
    "1. **Domain-driven features** - Creates meaningful composite scores (academic focus, professional focus)\n",
    "2. **Diversity metrics** - Captures tool and use case diversity effectively\n",
    "3. **Weighted scoring** - Device and internet quality use logical weights\n",
    "4. **Interaction terms** - Captures relationships between variables\n",
    "5. **Robust error handling** - Checks for column existence before creating features\n",
    "6. **Clear documentation** - Well-commented and structured\n",
    "\n",
    "### ⚠️ **AREAS FOR IMPROVEMENT:**\n",
    "1. **Potential missing features** - Some features might not exist after correlation optimization\n",
    "2. **Fixed weights** - Device/internet quality weights could be data-driven\n",
    "3. **Limited interaction terms** - Could explore more meaningful interactions\n",
    "4. **No normalization** - Composite scores might have different scales\n",
    "5. **Missing validation** - No checks for feature quality or predictive power\n",
    "\n",
    "### 🔧 **RECOMMENDATIONS:**\n",
    "1. **Add feature existence validation** for all referenced columns\n",
    "2. **Create data-driven weights** based on target correlation\n",
    "3. **Add normalization** for composite scores\n",
    "4. **Include more interaction terms** based on domain knowledge\n",
    "5. **Add feature quality validation** after creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf558791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing improved feature engineering function...\n",
      "=== IMPROVED ADVANCED FEATURE ENGINEERING ===\n",
      "✅ tech_adoption_score: Composite score from 2 features\n",
      "✅ tech_adoption_score_normalized: Normalized technology adoption score (0-1)\n",
      "✅ trust_x_usage: Interaction between Trust_in_AI_Tools and Daily_Usage_Hours\n",
      "✅ year_x_usage: Interaction between Year_of_Study and Daily_Usage_Hours\n",
      "✅ awareness_x_usage: Interaction between Awareness_Level and Daily_Usage_Hours\n",
      "✅ trust_x_awareness: Interaction between Trust_in_AI_Tools and Awareness_Level\n",
      "✅ experience_intensity: Study experience × usage intensity\n",
      "\n",
      "=== FEATURE QUALITY VALIDATION ===\n",
      "⚠️ Weak tech_adoption_score: corr=0.000, var=10.955, unique=14\n",
      "⚠️ Weak tech_adoption_score_normalized: corr=0.000, var=0.049, unique=14\n",
      "✅ Good trust_x_usage: corr=0.666, var=29.734, unique=159\n",
      "✅ Good year_x_usage: corr=0.688, var=18.448, unique=133\n",
      "✅ Good awareness_x_usage: corr=0.653, var=118.602, unique=274\n",
      "✅ Good trust_x_awareness: corr=0.015, var=172.532, unique=28\n",
      "✅ Good experience_intensity: corr=0.688, var=18.448, unique=133\n",
      "\n",
      "✅ Successfully created 7 new features\n",
      "📊 Dataset shape: (3614, 16) → (3614, 23)\n",
      "\n",
      "🎉 Success! Created 7 features\n"
     ]
    }
   ],
   "source": [
    "# 🔧 IMPROVED VERSION OF create_advanced_features\n",
    "def create_advanced_features_improved(data, target_col='Daily_Usage_Hours'):\n",
    "    \"\"\"\n",
    "    Create more sophisticated and robust features with validation\n",
    "    \"\"\"\n",
    "    print(\"=== IMPROVED ADVANCED FEATURE ENGINEERING ===\")\n",
    "    \n",
    "    data_enhanced = data.copy()\n",
    "    new_features_created = []\n",
    "    \n",
    "    # Helper function to safely create features\n",
    "    def safe_feature_creation(feature_name, feature_func, description):\n",
    "        try:\n",
    "            data_enhanced[feature_name] = feature_func()\n",
    "            new_features_created.append(feature_name)\n",
    "            print(f\"✅ {feature_name}: {description}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {feature_name}: Failed - {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    # 1. AI Tool Diversity Score (improved)\n",
    "    ai_tool_cols = [col for col in data.columns if col.startswith('ai_tool_')]\n",
    "    if ai_tool_cols:\n",
    "        safe_feature_creation(\n",
    "            'ai_tool_diversity',\n",
    "            lambda: data[ai_tool_cols].sum(axis=1),\n",
    "            f\"Sum of {len(ai_tool_cols)} AI tools used\"\n",
    "        )\n",
    "        \n",
    "        # Normalized diversity score\n",
    "        safe_feature_creation(\n",
    "            'ai_tool_diversity_normalized',\n",
    "            lambda: data[ai_tool_cols].sum(axis=1) / len(ai_tool_cols),\n",
    "            \"Normalized AI tool diversity (0-1)\"\n",
    "        )\n",
    "    \n",
    "    # 2. Use Case Diversity Score (improved)\n",
    "    use_case_cols = [col for col in data.columns if col.startswith('uses_')]\n",
    "    if use_case_cols:\n",
    "        safe_feature_creation(\n",
    "            'use_case_diversity',\n",
    "            lambda: data[use_case_cols].sum(axis=1),\n",
    "            f\"Sum of {len(use_case_cols)} use cases\"\n",
    "        )\n",
    "        \n",
    "        safe_feature_creation(\n",
    "            'use_case_diversity_normalized',\n",
    "            lambda: data[use_case_cols].sum(axis=1) / len(use_case_cols),\n",
    "            \"Normalized use case diversity (0-1)\"\n",
    "        )\n",
    "    \n",
    "    # 3. Technology Adoption Score (improved with validation)\n",
    "    tech_base_features = ['Trust_in_AI_Tools', 'Awareness_Level']\n",
    "    available_tech_features = [f for f in tech_base_features if f in data.columns]\n",
    "    \n",
    "    if available_tech_features:\n",
    "        if 'ai_tool_diversity' in data_enhanced.columns:\n",
    "            available_tech_features.append('ai_tool_diversity')\n",
    "        \n",
    "        safe_feature_creation(\n",
    "            'tech_adoption_score',\n",
    "            lambda: data_enhanced[available_tech_features].sum(axis=1),\n",
    "            f\"Composite score from {len(available_tech_features)} features\"\n",
    "        )\n",
    "        \n",
    "        # Normalized version\n",
    "        safe_feature_creation(\n",
    "            'tech_adoption_score_normalized',\n",
    "            lambda: (data_enhanced[available_tech_features].sum(axis=1) / \n",
    "                    data_enhanced[available_tech_features].sum(axis=1).max()),\n",
    "            \"Normalized technology adoption score (0-1)\"\n",
    "        )\n",
    "    \n",
    "    # 4. Academic Focus Score (improved with validation)\n",
    "    academic_uses = ['uses_assignments', 'uses_exam_preparation', 'uses_project_work']\n",
    "    available_academic = [f for f in academic_uses if f in data.columns]\n",
    "    \n",
    "    if available_academic:\n",
    "        safe_feature_creation(\n",
    "            'academic_focus',\n",
    "            lambda: data_enhanced[available_academic].sum(axis=1),\n",
    "            f\"Academic focus from {len(available_academic)} features\"\n",
    "        )\n",
    "        \n",
    "        safe_feature_creation(\n",
    "            'academic_focus_normalized',\n",
    "            lambda: data_enhanced[available_academic].sum(axis=1) / len(available_academic),\n",
    "            \"Normalized academic focus (0-1)\"\n",
    "        )\n",
    "    \n",
    "    # 5. Professional Focus Score (improved with validation)\n",
    "    professional_uses = ['uses_resume_writing', 'uses_content_writing', 'uses_coding_help']\n",
    "    available_professional = [f for f in professional_uses if f in data.columns]\n",
    "    \n",
    "    if available_professional:\n",
    "        safe_feature_creation(\n",
    "            'professional_focus',\n",
    "            lambda: data_enhanced[available_professional].sum(axis=1),\n",
    "            f\"Professional focus from {len(available_professional)} features\"\n",
    "        )\n",
    "        \n",
    "        safe_feature_creation(\n",
    "            'professional_focus_normalized',\n",
    "            lambda: data_enhanced[available_professional].sum(axis=1) / len(available_professional),\n",
    "            \"Normalized professional focus (0-1)\"\n",
    "        )\n",
    "    \n",
    "    # 6. Focus Balance Score (NEW)\n",
    "    if 'academic_focus' in data_enhanced.columns and 'professional_focus' in data_enhanced.columns:\n",
    "        safe_feature_creation(\n",
    "            'focus_balance',\n",
    "            lambda: abs(data_enhanced['academic_focus'] - data_enhanced['professional_focus']),\n",
    "            \"Balance between academic and professional focus\"\n",
    "        )\n",
    "    \n",
    "    # 7. Device Quality Score (improved with data-driven weights)\n",
    "    device_cols = [col for col in data.columns if col.startswith('device_used_')]\n",
    "    if len(device_cols) > 0:\n",
    "        # Calculate data-driven weights based on target correlation\n",
    "        if target_col in data.columns:\n",
    "            device_weights = {}\n",
    "            for col in device_cols:\n",
    "                corr_with_target = abs(data[col].corr(data[target_col]))\n",
    "                if 'laptop' in col.lower():\n",
    "                    device_weights[col] = corr_with_target * 3\n",
    "                elif 'tablet' in col.lower():\n",
    "                    device_weights[col] = corr_with_target * 2\n",
    "                else:\n",
    "                    device_weights[col] = corr_with_target * 1\n",
    "            \n",
    "            safe_feature_creation(\n",
    "                'device_quality_weighted',\n",
    "                lambda: sum(data_enhanced[col] * weight for col, weight in device_weights.items()),\n",
    "                f\"Data-driven weighted device quality from {len(device_cols)} devices\"\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to original logic\n",
    "            laptop_col = next((col for col in device_cols if 'laptop' in col.lower()), None)\n",
    "            tablet_col = next((col for col in device_cols if 'tablet' in col.lower()), None)\n",
    "            mobile_col = next((col for col in device_cols if 'mobile' in col.lower()), None)\n",
    "            \n",
    "            if laptop_col or tablet_col or mobile_col:\n",
    "                safe_feature_creation(\n",
    "                    'device_quality',\n",
    "                    lambda: ((data_enhanced[laptop_col] * 3 if laptop_col else 0) +\n",
    "                            (data_enhanced[tablet_col] * 2 if tablet_col else 0) +\n",
    "                            (data_enhanced[mobile_col] * 1 if mobile_col else 0)),\n",
    "                    \"Traditional weighted device quality\"\n",
    "                )\n",
    "    \n",
    "    # 8. Internet Quality Score (improved with validation)\n",
    "    internet_cols = [col for col in data.columns if col.startswith('internet_access_')]\n",
    "    if len(internet_cols) > 0:\n",
    "        high_col = next((col for col in internet_cols if 'high' in col.lower()), None)\n",
    "        medium_col = next((col for col in internet_cols if 'medium' in col.lower()), None)\n",
    "        poor_col = next((col for col in internet_cols if 'poor' in col.lower()), None)\n",
    "        \n",
    "        if high_col or medium_col or poor_col:\n",
    "            safe_feature_creation(\n",
    "                'internet_quality',\n",
    "                lambda: ((data_enhanced[high_col] * 3 if high_col else 0) +\n",
    "                        (data_enhanced[medium_col] * 2 if medium_col else 0) +\n",
    "                        (data_enhanced[poor_col] * 1 if poor_col else 0)),\n",
    "                \"Weighted internet quality score\"\n",
    "            )\n",
    "    \n",
    "    # 9. Enhanced Interaction Terms\n",
    "    interaction_pairs = [\n",
    "        ('Trust_in_AI_Tools', 'Daily_Usage_Hours', 'trust_x_usage'),\n",
    "        ('Year_of_Study', 'Daily_Usage_Hours', 'year_x_usage'),\n",
    "        ('Awareness_Level', 'Daily_Usage_Hours', 'awareness_x_usage'),\n",
    "        ('Trust_in_AI_Tools', 'Awareness_Level', 'trust_x_awareness'),\n",
    "    ]\n",
    "    \n",
    "    for feat1, feat2, new_name in interaction_pairs:\n",
    "        if feat1 in data.columns and feat2 in data.columns:\n",
    "            safe_feature_creation(\n",
    "                new_name,\n",
    "                lambda f1=feat1, f2=feat2: data_enhanced[f1] * data_enhanced[f2],\n",
    "                f\"Interaction between {feat1} and {feat2}\"\n",
    "            )\n",
    "    \n",
    "    # 10. Advanced Composite Features (NEW)\n",
    "    if 'ai_tool_diversity' in data_enhanced.columns and 'use_case_diversity' in data_enhanced.columns:\n",
    "        safe_feature_creation(\n",
    "            'overall_ai_engagement',\n",
    "            lambda: (data_enhanced['ai_tool_diversity'] + data_enhanced['use_case_diversity']) / 2,\n",
    "            \"Overall AI engagement score\"\n",
    "        )\n",
    "    \n",
    "    # 11. Experience-based Features (NEW)\n",
    "    if 'Year_of_Study' in data.columns and 'Daily_Usage_Hours' in data.columns:\n",
    "        safe_feature_creation(\n",
    "            'experience_intensity',\n",
    "            lambda: data_enhanced['Year_of_Study'] * data_enhanced['Daily_Usage_Hours'],\n",
    "            \"Study experience × usage intensity\"\n",
    "        )\n",
    "    \n",
    "    # 12. Feature Quality Validation\n",
    "    print(f\"\\n=== FEATURE QUALITY VALIDATION ===\")\n",
    "    if target_col in data.columns:\n",
    "        for feature in new_features_created:\n",
    "            if feature in data_enhanced.columns:\n",
    "                target_corr = abs(data_enhanced[feature].corr(data_enhanced[target_col]))\n",
    "                variance = data_enhanced[feature].var()\n",
    "                unique_vals = data_enhanced[feature].nunique()\n",
    "                \n",
    "                quality_score = \"✅ Good\" if target_corr > 0.01 and variance > 0 else \"⚠️ Weak\"\n",
    "                print(f\"{quality_score} {feature}: corr={target_corr:.3f}, var={variance:.3f}, unique={unique_vals}\")\n",
    "    \n",
    "    print(f\"\\n✅ Successfully created {len(new_features_created)} new features\")\n",
    "    print(f\"📊 Dataset shape: {data.shape} → {data_enhanced.shape}\")\n",
    "    \n",
    "    return data_enhanced, new_features_created\n",
    "\n",
    "# Test the improved function\n",
    "print(\"Testing improved feature engineering function...\")\n",
    "try:\n",
    "    data_enhanced_improved, new_features_improved = create_advanced_features_improved(data)\n",
    "    print(f\"\\n🎉 Success! Created {len(new_features_improved)} features\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Will fall back to original function if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f2b144ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON: ORIGINAL vs IMPROVED FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "🔵 ORIGINAL FUNCTION RESULTS:\n",
      "❌ Error in comparison: name 'create_advanced_features' is not defined\n",
      "\n",
      "================================================================================\n",
      "🎯 FINAL RECOMMENDATION\n",
      "================================================================================\n",
      "\n",
      "📝 ASSESSMENT OF ORIGINAL create_advanced_features():\n",
      "\n",
      "✅ STRENGTHS:\n",
      "• Domain-driven feature creation (academic vs professional focus)\n",
      "• Logical composite scores (diversity, quality scores)\n",
      "• Appropriate weighting for device/internet quality\n",
      "• Clean code structure and documentation\n",
      "• Basic interaction terms\n",
      "\n",
      "⚠️ AREAS FOR IMPROVEMENT:\n",
      "• Missing validation for feature existence (problematic after correlation optimization)\n",
      "• Fixed weights instead of data-driven weights\n",
      "• Limited interaction terms\n",
      "• No normalization of composite scores\n",
      "• No quality validation after feature creation\n",
      "\n",
      "🔧 RECOMMENDATIONS:\n",
      "1. ADD VALIDATION: Check if referenced columns exist before creating features\n",
      "2. USE DATA-DRIVEN WEIGHTS: Base weights on target correlation, not fixed values\n",
      "3. NORMALIZE SCORES: Add normalized versions of composite scores (0-1 range)\n",
      "4. EXPAND INTERACTIONS: Include more meaningful interaction terms\n",
      "5. VALIDATE QUALITY: Check feature quality after creation\n",
      "\n",
      "💡 VERDICT: The original function is GOOD but needs ROBUSTNESS IMPROVEMENTS\n",
      "especially when working with optimized datasets where some features might be removed.\n",
      "\n",
      "🎖️ RECOMMENDED ACTION:\n",
      "Use the IMPROVED VERSION for production, as it:\n",
      "• Handles missing features gracefully\n",
      "• Creates more robust and normalized features\n",
      "• Provides better quality validation\n",
      "• Generates additional meaningful features\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 📊 COMPARISON: Original vs Improved Feature Engineering\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON: ORIGINAL vs IMPROVED FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare the functions\n",
    "try:\n",
    "    # Original function results\n",
    "    print(\"\\n🔵 ORIGINAL FUNCTION RESULTS:\")\n",
    "    data_original, original_features = create_advanced_features(data)\n",
    "    print(f\"   Created features: {len(original_features)}\")\n",
    "    print(f\"   Dataset shape: {data.shape} → {data_original.shape}\")\n",
    "    \n",
    "    # Improved function results\n",
    "    print(\"\\n🟢 IMPROVED FUNCTION RESULTS:\")\n",
    "    data_improved, improved_features = create_advanced_features_improved(data)\n",
    "    print(f\"   Created features: {len(improved_features)}\")\n",
    "    print(f\"   Dataset shape: {data.shape} → {data_improved.shape}\")\n",
    "    \n",
    "    # Feature comparison\n",
    "    print(f\"\\n📋 FEATURE COMPARISON:\")\n",
    "    original_set = set(original_features)\n",
    "    improved_set = set(improved_features)\n",
    "    \n",
    "    common_features = original_set & improved_set\n",
    "    only_original = original_set - improved_set\n",
    "    only_improved = improved_set - original_set\n",
    "    \n",
    "    print(f\"   Common features: {len(common_features)}\")\n",
    "    print(f\"   Only in original: {len(only_original)}\")\n",
    "    print(f\"   Only in improved: {len(only_improved)}\")\n",
    "    \n",
    "    if only_improved:\n",
    "        print(f\"\\n✨ NEW FEATURES IN IMPROVED VERSION:\")\n",
    "        for i, feature in enumerate(sorted(only_improved), 1):\n",
    "            print(f\"   {i:2d}. {feature}\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    if 'Daily_Usage_Hours' in data.columns:\n",
    "        print(f\"\\n📈 QUALITY ASSESSMENT:\")\n",
    "        \n",
    "        # Original features quality\n",
    "        original_target_corrs = []\n",
    "        for feature in original_features:\n",
    "            if feature in data_original.columns:\n",
    "                corr = abs(data_original[feature].corr(data_original['Daily_Usage_Hours']))\n",
    "                original_target_corrs.append(corr)\n",
    "        \n",
    "        # Improved features quality\n",
    "        improved_target_corrs = []\n",
    "        for feature in improved_features:\n",
    "            if feature in data_improved.columns:\n",
    "                corr = abs(data_improved[feature].corr(data_improved['Daily_Usage_Hours']))\n",
    "                improved_target_corrs.append(corr)\n",
    "        \n",
    "        if original_target_corrs and improved_target_corrs:\n",
    "            print(f\"   Original avg target correlation: {np.mean(original_target_corrs):.4f}\")\n",
    "            print(f\"   Improved avg target correlation: {np.mean(improved_target_corrs):.4f}\")\n",
    "            print(f\"   Improvement: {np.mean(improved_target_corrs) - np.mean(original_target_corrs):+.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in comparison: {e}\")\n",
    "\n",
    "# Final recommendation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🎯 FINAL RECOMMENDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "📝 ASSESSMENT OF ORIGINAL create_advanced_features():\n",
    "\n",
    "✅ STRENGTHS:\n",
    "• Domain-driven feature creation (academic vs professional focus)\n",
    "• Logical composite scores (diversity, quality scores)\n",
    "• Appropriate weighting for device/internet quality\n",
    "• Clean code structure and documentation\n",
    "• Basic interaction terms\n",
    "\n",
    "⚠️ AREAS FOR IMPROVEMENT:\n",
    "• Missing validation for feature existence (problematic after correlation optimization)\n",
    "• Fixed weights instead of data-driven weights\n",
    "• Limited interaction terms\n",
    "• No normalization of composite scores\n",
    "• No quality validation after feature creation\n",
    "\n",
    "🔧 RECOMMENDATIONS:\n",
    "1. ADD VALIDATION: Check if referenced columns exist before creating features\n",
    "2. USE DATA-DRIVEN WEIGHTS: Base weights on target correlation, not fixed values\n",
    "3. NORMALIZE SCORES: Add normalized versions of composite scores (0-1 range)\n",
    "4. EXPAND INTERACTIONS: Include more meaningful interaction terms\n",
    "5. VALIDATE QUALITY: Check feature quality after creation\n",
    "\n",
    "💡 VERDICT: The original function is GOOD but needs ROBUSTNESS IMPROVEMENTS\n",
    "especially when working with optimized datasets where some features might be removed.\n",
    "\n",
    "🎖️ RECOMMENDED ACTION:\n",
    "Use the IMPROVED VERSION for production, as it:\n",
    "• Handles missing features gracefully\n",
    "• Creates more robust and normalized features\n",
    "• Provides better quality validation\n",
    "• Generates additional meaningful features\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c02e320d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Study</th>\n",
       "      <th>Daily_Usage_Hours</th>\n",
       "      <th>Trust_in_AI_Tools</th>\n",
       "      <th>Impact_on_Grades</th>\n",
       "      <th>Awareness_Level</th>\n",
       "      <th>uses_coding_help</th>\n",
       "      <th>uses_assignments</th>\n",
       "      <th>uses_project_work</th>\n",
       "      <th>uses_mcq_practice</th>\n",
       "      <th>uses_exam_preparation</th>\n",
       "      <th>...</th>\n",
       "      <th>Willing_to_Pay_for_Access_encoded</th>\n",
       "      <th>ai_tool_diversity</th>\n",
       "      <th>use_case_diversity</th>\n",
       "      <th>tech_adoption_score</th>\n",
       "      <th>academic_focus</th>\n",
       "      <th>professional_focus</th>\n",
       "      <th>device_quality</th>\n",
       "      <th>internet_quality</th>\n",
       "      <th>trust_x_usage</th>\n",
       "      <th>year_x_usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>14.5</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Study  Daily_Usage_Hours  Trust_in_AI_Tools  Impact_on_Grades  \\\n",
       "0              4                0.9                  2                 2   \n",
       "1              2                3.4                  3                -3   \n",
       "2              2                3.6                  5                 0   \n",
       "3              2                2.9                  5                 2   \n",
       "4              1                0.9                  1                 3   \n",
       "\n",
       "   Awareness_Level  uses_coding_help  uses_assignments  uses_project_work  \\\n",
       "0                9                 1                 1                  0   \n",
       "1                6                 0                 0                  0   \n",
       "2                1                 0                 0                  1   \n",
       "3                5                 0                 0                  0   \n",
       "4                8                 0                 0                  0   \n",
       "\n",
       "   uses_mcq_practice  uses_exam_preparation  ...  \\\n",
       "0                  0                      0  ...   \n",
       "1                  0                      0  ...   \n",
       "2                  1                      0  ...   \n",
       "3                  0                      0  ...   \n",
       "4                  0                      0  ...   \n",
       "\n",
       "   Willing_to_Pay_for_Access_encoded  ai_tool_diversity  use_case_diversity  \\\n",
       "0                                  1                  1                   2   \n",
       "1                                  0                  1                   1   \n",
       "2                                  0                  1                   2   \n",
       "3                                  0                  1                   1   \n",
       "4                                  1                  1                   2   \n",
       "\n",
       "   tech_adoption_score  academic_focus  professional_focus  device_quality  \\\n",
       "0                   12               1                   1               1   \n",
       "1                   10               0                   0               3   \n",
       "2                    7               1                   0               2   \n",
       "3                   11               0                   1               3   \n",
       "4                   10               0                   1               3   \n",
       "\n",
       "   internet_quality  trust_x_usage  year_x_usage  \n",
       "0                 1            1.8           3.6  \n",
       "1                 1           10.2           6.8  \n",
       "2                 1           18.0           7.2  \n",
       "3                 3           14.5           5.8  \n",
       "4                 2            0.9           0.9  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 5\n",
    "data_enhanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b916e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year_of_Study',\n",
       " 'Daily_Usage_Hours',\n",
       " 'Trust_in_AI_Tools',\n",
       " 'Impact_on_Grades',\n",
       " 'Awareness_Level',\n",
       " 'uses_coding_help',\n",
       " 'uses_assignments',\n",
       " 'uses_project_work',\n",
       " 'uses_mcq_practice',\n",
       " 'uses_exam_preparation',\n",
       " 'uses_doubt_solving',\n",
       " 'uses_resume_writing',\n",
       " 'uses_content_writing',\n",
       " 'uses_learning_new_topics',\n",
       " 'uses_notes',\n",
       " 'ai_tool_bard',\n",
       " 'ai_tool_chatgpt',\n",
       " 'ai_tool_claude',\n",
       " 'ai_tool_copilot',\n",
       " 'ai_tool_gemini',\n",
       " 'ai_tool_midjourney',\n",
       " 'ai_tool_other',\n",
       " 'preferred_ai_tool_Bard',\n",
       " 'preferred_ai_tool_ChatGPT',\n",
       " 'preferred_ai_tool_Claude',\n",
       " 'preferred_ai_tool_Copilot',\n",
       " 'preferred_ai_tool_Gemini',\n",
       " 'preferred_ai_tool_Other',\n",
       " 'device_used_Laptop',\n",
       " 'device_used_Mobile',\n",
       " 'device_used_Tablet',\n",
       " 'internet_access_High',\n",
       " 'internet_access_Medium',\n",
       " 'internet_access_Poor',\n",
       " 'Do_Professors_Allow_Use_encoded',\n",
       " 'Willing_to_Pay_for_Access_encoded',\n",
       " 'ai_tool_diversity',\n",
       " 'use_case_diversity',\n",
       " 'tech_adoption_score',\n",
       " 'academic_focus',\n",
       " 'professional_focus',\n",
       " 'device_quality',\n",
       " 'internet_quality',\n",
       " 'trust_x_usage',\n",
       " 'year_x_usage']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names\n",
    "data_enhanced.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dfe8a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA QUALITY VALIDATION ===\n",
      "✅ No missing values\n",
      "✅ No constant features\n",
      "\n",
      "Feature Distribution Summary:\n",
      "Binary features: 31\n",
      "Categorical features: 3\n",
      "Continuous features: 2\n",
      "✅ No extreme multicollinearity\n"
     ]
    }
   ],
   "source": [
    "# 3. Comprehensive data validation\n",
    "def validate_data_quality(data):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality check\n",
    "    \"\"\"\n",
    "    print(\"=== DATA QUALITY VALIDATION ===\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_counts = data.isnull().sum()\n",
    "    if missing_counts.sum() > 0:\n",
    "        print(f\"⚠️ Missing values found:\")\n",
    "        print(missing_counts[missing_counts > 0])\n",
    "    else:\n",
    "        print(\"✅ No missing values\")\n",
    "    \n",
    "    # Check for constant features\n",
    "    constant_features = []\n",
    "    for col in data.columns:\n",
    "        if data[col].nunique() <= 1:\n",
    "            constant_features.append(col)\n",
    "    \n",
    "    if constant_features:\n",
    "        print(f\"⚠️ Constant features (remove these): {constant_features}\")\n",
    "    else:\n",
    "        print(\"✅ No constant features\")\n",
    "    \n",
    "    # Check feature distributions\n",
    "    print(f\"\\nFeature Distribution Summary:\")\n",
    "    print(f\"Binary features: {sum(data[col].nunique() == 2 for col in data.columns)}\")\n",
    "    print(f\"Categorical features: {sum(2 < data[col].nunique() <= 10 for col in data.columns)}\")\n",
    "    print(f\"Continuous features: {sum(data[col].nunique() > 10 for col in data.columns)}\")\n",
    "    \n",
    "    # Check for multicollinearity\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    corr_matrix = data[numeric_cols].corr()\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    high_corr_count = 0\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "                high_corr_count += 1\n",
    "    \n",
    "    if high_corr_count > 0:\n",
    "        print(f\"⚠️ {high_corr_count} highly correlated pairs (>0.9)\")\n",
    "    else:\n",
    "        print(\"✅ No extreme multicollinearity\")\n",
    "    \n",
    "    return constant_features\n",
    "\n",
    "# Validate data quality\n",
    "issues = validate_data_quality(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee6b8a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADVANCED MODELING PIPELINE ===\n",
      "Selected 20 features using statistical selection\n",
      "\n",
      "Optimizing Random Forest...\n",
      "  Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "  CV R²: 0.8780 (±0.0238)\n",
      "\n",
      "Optimizing Gradient Boosting...\n",
      "  Best parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "  CV R²: 0.8593 (±0.0156)\n",
      "\n",
      "Optimizing ElasticNet...\n",
      "  Best parameters: {'alpha': 0.1, 'l1_ratio': 0.1}\n",
      "  CV R²: 0.6348 (±0.0241)\n"
     ]
    }
   ],
   "source": [
    "# 4. Advanced modeling approach\n",
    "def advanced_modeling_pipeline(data, target_col='Daily_Usage_Hours'):\n",
    "    \"\"\"\n",
    "    Comprehensive modeling pipeline\n",
    "    \"\"\"\n",
    "    print(\"=== ADVANCED MODELING PIPELINE ===\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # 1. Feature Selection\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    # Statistical feature selection\n",
    "    selector = SelectKBest(score_func=f_regression, k=20)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features using statistical selection\")\n",
    "    \n",
    "    # 2. Model with hyperparameter tuning\n",
    "    from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    \n",
    "    models = {\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'learning_rate': [0.1, 0.05],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        },\n",
    "        'ElasticNet': {\n",
    "            'model': ElasticNet(random_state=42),\n",
    "            'params': {\n",
    "                'alpha': [0.1, 1.0, 10.0],\n",
    "                'l1_ratio': [0.1, 0.5, 0.9]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Grid search for best models\n",
    "    best_models = {}\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X[selected_features], y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    for name, model_info in models.items():\n",
    "        print(f\"\\nOptimizing {name}...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            model_info['model'], \n",
    "            model_info['params'],\n",
    "            cv=5,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='r2')\n",
    "        \n",
    "        print(f\"  Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"  CV R²: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "        \n",
    "        best_models[name] = {\n",
    "            'model': best_model,\n",
    "            'cv_score': cv_scores.mean(),\n",
    "            'params': grid_search.best_params_\n",
    "        }\n",
    "    \n",
    "    return best_models, selected_features\n",
    "\n",
    "# Run advanced modeling\n",
    "best_models, top_features = advanced_modeling_pipeline(data_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1a98815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Impact_on_Grades', 'uses_project_work', 'uses_mcq_practice',\n",
       "       'uses_exam_preparation', 'uses_doubt_solving', 'uses_resume_writing',\n",
       "       'uses_learning_new_topics', 'ai_tool_chatgpt', 'ai_tool_copilot',\n",
       "       'ai_tool_other', 'preferred_ai_tool_Bard', 'preferred_ai_tool_ChatGPT',\n",
       "       'preferred_ai_tool_Claude', 'preferred_ai_tool_Other',\n",
       "       'Willing_to_Pay_for_Access_encoded', 'ai_tool_diversity',\n",
       "       'use_case_diversity', 'professional_focus', 'trust_x_usage',\n",
       "       'year_x_usage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b37cb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = data_enhanced.drop(columns=[\"Daily_Usage_Hours\"])\n",
    "y = data_enhanced[\"Daily_Usage_Hours\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88d7c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f386967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(score_func=f_regression, k=20)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb24d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X[selected_features], y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75ff8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c71c1bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL INTERPRETABILITY ===\n",
      "Top 10 Most Important Features:\n",
      " 1. year_x_usage                   0.6162\n",
      " 2. trust_x_usage                  0.2748\n",
      " 3. Impact_on_Grades               0.0252\n",
      " 4. ai_tool_diversity              0.0108\n",
      " 5. use_case_diversity             0.0103\n",
      " 6. preferred_ai_tool_ChatGPT      0.0071\n",
      " 7. professional_focus             0.0061\n",
      " 8. uses_learning_new_topics       0.0050\n",
      " 9. uses_mcq_practice              0.0049\n",
      "10. ai_tool_copilot                0.0048\n",
      "\n",
      "Model Performance:\n",
      "Mean Absolute Error: 0.2159\n",
      "R² Score: 0.8850\n",
      "Predictions within 1 std: 82.3%\n",
      "Predictions within 2 std: 93.2%\n"
     ]
    }
   ],
   "source": [
    "# 5. Model interpretability analysis\n",
    "def model_interpretability(model, feature_names, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Analyze model interpretability\n",
    "    \"\"\"\n",
    "    print(\"=== MODEL INTERPRETABILITY ===\")\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"Top 10 Most Important Features:\")\n",
    "        for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):\n",
    "            print(f\"{i:2d}. {row['feature']:<30} {row['importance']:.4f}\")\n",
    "    \n",
    "    # Model performance analysis\n",
    "    predictions = model.predict(X_test)\n",
    "    residuals = y_test - predictions\n",
    "    \n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"Mean Absolute Error: {np.mean(np.abs(residuals)):.4f}\")\n",
    "    print(f\"R² Score: {r2_score(y_test, predictions):.4f}\")\n",
    "    \n",
    "    # Prediction intervals\n",
    "    within_1_std = np.sum(np.abs(residuals) <= np.std(residuals)) / len(residuals)\n",
    "    within_2_std = np.sum(np.abs(residuals) <= 2 * np.std(residuals)) / len(residuals)\n",
    "    \n",
    "    print(f\"Predictions within 1 std: {within_1_std:.1%}\")\n",
    "    print(f\"Predictions within 2 std: {within_2_std:.1%}\")\n",
    "    \n",
    "    return importance_df if hasattr(model, 'feature_importances_') else None\n",
    "\n",
    "# Apply interpretability analysis\n",
    "if best_models:\n",
    "    best_model_name = max(best_models.keys(), key=lambda x: best_models[x]['cv_score'])\n",
    "    interpretability = model_interpretability(\n",
    "        best_models[best_model_name]['model'], \n",
    "        top_features, \n",
    "        X_test, \n",
    "        y_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5f0a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def correlation_based_selection(data, target_col='Daily_Usage_Hours', min_target_corr=0.05, max_feature_corr=0.8):\n",
    "#     \"\"\"\n",
    "#     Select features based on correlation with target and between features\n",
    "#     \"\"\"\n",
    "#     print(f\"=== CORRELATION-BASED FEATURE SELECTION ===\")\n",
    "#     print(f\"Min target correlation: {min_target_corr}\")\n",
    "#     print(f\"Max inter-feature correlation: {max_feature_corr}\")\n",
    "    \n",
    "#     X = data.drop(columns=[target_col])\n",
    "#     y = data[target_col]\n",
    "    \n",
    "#     # Step 1: Filter by correlation with target\n",
    "#     target_correlations = X.corrwith(y).abs()\n",
    "#     high_target_corr_features = target_correlations[target_correlations >= min_target_corr].index.tolist()\n",
    "    \n",
    "#     print(f\"\\nStep 1: Features with target correlation >= {min_target_corr}\")\n",
    "#     print(f\"Selected {len(high_target_corr_features)} out of {len(X.columns)} features\")\n",
    "    \n",
    "#     if len(high_target_corr_features) == 0:\n",
    "#         print(\"No features meet target correlation threshold!\")\n",
    "#         return []\n",
    "    \n",
    "#     # Step 2: Remove highly correlated features among selected ones\n",
    "#     X_filtered = X[high_target_corr_features]\n",
    "#     corr_matrix = X_filtered.corr()\n",
    "    \n",
    "#     # Find pairs of highly correlated features\n",
    "#     to_remove = set()\n",
    "#     for i in range(len(corr_matrix.columns)):\n",
    "#         for j in range(i+1, len(corr_matrix.columns)):\n",
    "#             if abs(corr_matrix.iloc[i, j]) > max_feature_corr:\n",
    "#                 feat1, feat2 = corr_matrix.columns[i], corr_matrix.columns[j]\n",
    "                \n",
    "#                 # Keep the one with higher target correlation\n",
    "#                 if target_correlations[feat1] >= target_correlations[feat2]:\n",
    "#                     to_remove.add(feat2)\n",
    "#                 else:\n",
    "#                     to_remove.add(feat1)\n",
    "    \n",
    "#     final_features = [f for f in high_target_corr_features if f not in to_remove]\n",
    "    \n",
    "#     print(f\"\\nStep 2: Remove inter-correlated features (>{max_feature_corr})\")\n",
    "#     print(f\"Removed {len(to_remove)} features: {list(to_remove)}\")\n",
    "#     print(f\"Final selection: {len(final_features)} features\")\n",
    "    \n",
    "#     # Show final selected features with their target correlations\n",
    "#     print(f\"\\nFinal Selected Features:\")\n",
    "#     final_target_corrs = target_correlations[final_features].sort_values(ascending=False)\n",
    "#     for i, (feature, corr) in enumerate(final_target_corrs.items(), 1):\n",
    "#         print(f\"{i:2d}. {feature:<35} {corr:.4f}\")\n",
    "    \n",
    "#     return final_features\n",
    "\n",
    "# # Apply correlation-based selection\n",
    "# corr_selected_features = correlation_based_selection(data_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33785c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_final_feature_set(data, statistical_features, correlation_features, target_col='Daily_Usage_Hours'):\n",
    "#     \"\"\"\n",
    "#     Combine different feature selection methods to create final dataset\n",
    "#     \"\"\"\n",
    "#     print(\"=== CREATING FINAL FEATURE SET ===\")\n",
    "    \n",
    "#     # Take union of both methods\n",
    "#     all_selected = list(set(statistical_features + correlation_features))\n",
    "    \n",
    "#     # Always include target variable\n",
    "#     final_features = all_selected + [target_col]\n",
    "    \n",
    "#     # Create final dataset\n",
    "#     data_final = data[final_features].copy()\n",
    "    \n",
    "#     print(f\"Features from statistical selection: {len(statistical_features)}\")\n",
    "#     print(f\"Features from correlation selection: {len(correlation_features)}\")\n",
    "#     print(f\"Overlap between methods: {len(set(statistical_features) & set(correlation_features))}\")\n",
    "#     print(f\"Final feature set size: {len(all_selected)} features + target\")\n",
    "    \n",
    "#     print(f\"\\nFinal Selected Features:\")\n",
    "#     for i, feature in enumerate(sorted(all_selected), 1):\n",
    "#         in_stat = \"S\" if feature in statistical_features else \" \"\n",
    "#         in_corr = \"C\" if feature in correlation_features else \" \"\n",
    "#         print(f\"{i:2d}. [{in_stat}{in_corr}] {feature}\")\n",
    "    \n",
    "#     print(f\"\\nLegend: [S] = Statistical selection, [C] = Correlation selection\")\n",
    "    \n",
    "#     # Final correlation check\n",
    "#     final_corr_matrix = data_final.drop(columns=[target_col]).corr()\n",
    "#     high_corr_remaining = 0\n",
    "#     for i in range(len(final_corr_matrix.columns)):\n",
    "#         for j in range(i+1, len(final_corr_matrix.columns)):\n",
    "#             if abs(final_corr_matrix.iloc[i, j]) > 0.8:\n",
    "#                 high_corr_remaining += 1\n",
    "    \n",
    "#     print(f\"\\nFinal validation:\")\n",
    "#     print(f\"Remaining high correlations (>0.8): {high_corr_remaining}\")\n",
    "#     print(f\"Original dataset: {data.shape[1]} features\")\n",
    "#     print(f\"Final dataset: {data_final.shape[1]} features\")\n",
    "#     print(f\"Reduction: {((data.shape[1] - data_final.shape[1]) / data.shape[1] * 100):.1f}%\")\n",
    "    \n",
    "#     return data_final, all_selected\n",
    "\n",
    "# # Create final dataset\n",
    "# data_final_selected, final_feature_list = create_final_feature_set(\n",
    "#     data_reduced, selected_features, corr_selected_features\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81c35449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5\n",
    "# data_final_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "722567a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f23dbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_feature_selection_results(original_data, final_data, target_col='Daily_Usage_Hours'):\n",
    "#     \"\"\"\n",
    "#     Create visualizations to show the impact of feature selection\n",
    "#     \"\"\"\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "#     # 1. Feature count comparison\n",
    "#     original_features = original_data.shape[1] - 1  # exclude target\n",
    "#     final_features = final_data.shape[1] - 1  # exclude target\n",
    "    \n",
    "#     axes[0,0].bar(['Original', 'Final'], [original_features, final_features], \n",
    "#                   color=['lightcoral', 'lightgreen'])\n",
    "#     axes[0,0].set_title('Feature Count: Before vs After Selection')\n",
    "#     axes[0,0].set_ylabel('Number of Features')\n",
    "#     for i, v in enumerate([original_features, final_features]):\n",
    "#         axes[0,0].text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "#     # 2. Correlation heatmap of final features\n",
    "#     final_corr = final_data.drop(columns=[target_col]).corr()\n",
    "#     im = axes[0,1].imshow(final_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "#     axes[0,1].set_title('Correlation Matrix: Final Selected Features')\n",
    "#     axes[0,1].set_xticks(range(len(final_corr.columns)))\n",
    "#     axes[0,1].set_yticks(range(len(final_corr.columns)))\n",
    "#     axes[0,1].set_xticklabels([f[:10] + '...' if len(f) > 10 else f \n",
    "#                               for f in final_corr.columns], rotation=45, ha='right')\n",
    "#     axes[0,1].set_yticklabels([f[:10] + '...' if len(f) > 10 else f \n",
    "#                               for f in final_corr.columns])\n",
    "#     plt.colorbar(im, ax=axes[0,1], shrink=0.6)\n",
    "    \n",
    "#     # 3. Target correlations comparison\n",
    "#     original_target_corr = original_data.drop(columns=[target_col]).corrwith(original_data[target_col]).abs()\n",
    "#     final_target_corr = final_data.drop(columns=[target_col]).corrwith(final_data[target_col]).abs()\n",
    "    \n",
    "#     axes[1,0].hist(original_target_corr, bins=20, alpha=0.7, label='Original', color='lightcoral')\n",
    "#     axes[1,0].hist(final_target_corr, bins=10, alpha=0.7, label='Selected', color='lightgreen')\n",
    "#     axes[1,0].set_xlabel('Absolute Correlation with Target')\n",
    "#     axes[1,0].set_ylabel('Number of Features')\n",
    "#     axes[1,0].set_title('Distribution of Target Correlations')\n",
    "#     axes[1,0].legend()\n",
    "    \n",
    "#     # 4. Top features by target correlation\n",
    "#     top_features = final_target_corr.sort_values(ascending=False).head(10)\n",
    "#     y_pos = np.arange(len(top_features))\n",
    "    \n",
    "#     axes[1,1].barh(y_pos, top_features.values, color='lightblue')\n",
    "#     axes[1,1].set_yticks(y_pos)\n",
    "#     axes[1,1].set_yticklabels([f[:15] + '...' if len(f) > 15 else f \n",
    "#                               for f in top_features.index], fontsize=9)\n",
    "#     axes[1,1].set_xlabel('Absolute Correlation with Target')\n",
    "#     axes[1,1].set_title('Top 10 Selected Features by Target Correlation')\n",
    "#     axes[1,1].invert_yaxis()\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Summary statistics\n",
    "#     print(f\"\\n=== FEATURE SELECTION SUMMARY ===\")\n",
    "#     print(f\"Original features: {original_features}\")\n",
    "#     print(f\"Selected features: {final_features}\")\n",
    "#     print(f\"Reduction: {original_features - final_features} features ({((original_features - final_features) / original_features * 100):.1f}%)\")\n",
    "#     print(f\"Mean target correlation (original): {original_target_corr.mean():.4f}\")\n",
    "#     print(f\"Mean target correlation (selected): {final_target_corr.mean():.4f}\")\n",
    "#     print(f\"Max inter-feature correlation (selected): {final_corr.abs().max().max():.4f}\")\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_feature_selection_results(data, data_final_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "031ed35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_feature_selection_with_models(original_data, selected_data, target_col='Daily_Usage_Hours'):\n",
    "#     \"\"\"\n",
    "#     Compare model performance before and after feature selection\n",
    "#     \"\"\"\n",
    "#     print(\"=== VALIDATING FEATURE SELECTION WITH MODEL PERFORMANCE ===\")\n",
    "    \n",
    "#     from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#     from sklearn.ensemble import RandomForestRegressor\n",
    "#     from sklearn.linear_model import Ridge\n",
    "#     from sklearn.metrics import r2_score, mean_squared_error\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     for name, data in [('Original', original_data), ('Selected', selected_data)]:\n",
    "#         X = data.drop(columns=[target_col])\n",
    "#         y = data[target_col]\n",
    "        \n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "#         # Random Forest\n",
    "#         rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "#         rf_cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='r2')\n",
    "#         rf.fit(X_train, y_train)\n",
    "#         rf_pred = rf.predict(X_test)\n",
    "#         rf_r2 = r2_score(y_test, rf_pred)\n",
    "        \n",
    "#         # Ridge Regression\n",
    "#         ridge = Ridge(alpha=1.0)\n",
    "#         ridge_cv_scores = cross_val_score(ridge, X_train, y_train, cv=5, scoring='r2')\n",
    "#         ridge.fit(X_train, y_train)\n",
    "#         ridge_pred = ridge.predict(X_test)\n",
    "#         ridge_r2 = r2_score(y_test, ridge_pred)\n",
    "        \n",
    "#         results.append({\n",
    "#             'Dataset': name,\n",
    "#             'Features': X.shape[1],\n",
    "#             'RF_CV_R2': rf_cv_scores.mean(),\n",
    "#             'RF_Test_R2': rf_r2,\n",
    "#             'Ridge_CV_R2': ridge_cv_scores.mean(),\n",
    "#             'Ridge_Test_R2': ridge_r2\n",
    "#         })\n",
    "        \n",
    "#         print(f\"\\n{name} Dataset ({X.shape[1]} features):\")\n",
    "#         print(f\"  Random Forest - CV R²: {rf_cv_scores.mean():.4f}, Test R²: {rf_r2:.4f}\")\n",
    "#         print(f\"  Ridge Regression - CV R²: {ridge_cv_scores.mean():.4f}, Test R²: {ridge_r2:.4f}\")\n",
    "    \n",
    "#     results_df = pd.DataFrame(results)\n",
    "#     print(f\"\\n{results_df}\")\n",
    "    \n",
    "#     return results_df\n",
    "\n",
    "# # Validate feature selection\n",
    "# validation_results = validate_feature_selection_with_models(data, data_final_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef96b8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 DATASET SELECTION GUIDE FOR MODEL TRAINING\n",
      "============================================================\n",
      "\n",
      "📊 AVAILABLE DATASETS:\n",
      "✅ Students_Cleaned_Encoded_v1.csv\n",
      "   Basic encoded dataset (36 features)\n",
      "   Shape: (3614, 36)\n",
      "   Features: 35 (+ target)\n",
      "\n",
      "✅ Students_Cleaned_Encoded_full.csv\n",
      "   Complete dataset with all features\n",
      "   Shape: (3614, 36)\n",
      "   Features: 35 (+ target)\n",
      "\n",
      "✅ Students_Cleaned_Encoded_selected_20.csv\n",
      "   Top 20 features by importance\n",
      "   Shape: (3614, 21)\n",
      "   Features: 20 (+ target)\n",
      "\n",
      "✅ Students_Cleaned_Encoded_statistical.csv\n",
      "   Statistically selected features\n",
      "   Shape: (3614, 21)\n",
      "   Features: 20 (+ target)\n",
      "\n",
      "✅ Students_Cleaned_Encoded_pca_ready.csv\n",
      "   Numeric features ready for PCA\n",
      "   Shape: (3614, 36)\n",
      "   Features: 35 (+ target)\n",
      "\n",
      "🏆 RECOMMENDATIONS BY USE CASE:\n",
      "----------------------------------------\n",
      "\n",
      "1. 🚀 INITIAL MODEL DEVELOPMENT:\n",
      "   Dataset: Students_Cleaned_Encoded_selected_20.csv\n",
      "   Why: Best performance (R² = 0.5745) with optimal feature count\n",
      "   Use for: Quick prototyping, baseline models, feature importance analysis\n",
      "\n",
      "2. 🎯 PRODUCTION MODELS:\n",
      "   Dataset: Students_Cleaned_Encoded_v1.csv\n",
      "   Why: Balanced performance with well-tested features\n",
      "   Use for: Final models, deployment, consistent results\n",
      "\n",
      "3. 🔬 COMPREHENSIVE ANALYSIS:\n",
      "   Dataset: Students_Cleaned_Encoded_full.csv\n",
      "   Why: All features including advanced engineered features\n",
      "   Use for: Deep analysis, feature exploration, ensemble methods\n",
      "\n",
      "4. ⚡ FAST TRAINING:\n",
      "   Dataset: Students_Cleaned_Encoded_statistical.csv\n",
      "   Why: Statistically selected features for efficiency\n",
      "   Use for: Large-scale experiments, hyperparameter tuning\n",
      "\n",
      "5. 📈 DIMENSIONALITY REDUCTION:\n",
      "   Dataset: Students_Cleaned_Encoded_pca_ready.csv\n",
      "   Why: Prepared for PCA, standardized features\n",
      "   Use for: PCA, t-SNE, clustering, linear models\n",
      "\n",
      "============================================================\n",
      "🎖️  FINAL RECOMMENDATION:\n",
      "============================================================\n",
      "For MOST USERS: Start with 'Students_Cleaned_Encoded_selected_20.csv'\n",
      "• Best performance-to-complexity ratio\n",
      "• Fastest training time\n",
      "• Good interpretability\n",
      "• Reduced overfitting risk\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATASET SELECTION GUIDE FOR MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🎯 DATASET SELECTION GUIDE FOR MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check available datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "datasets = {\n",
    "    'Students_Cleaned_Encoded_v1.csv': 'Basic encoded dataset (36 features)',\n",
    "    'Students_Cleaned_Encoded_full.csv': 'Complete dataset with all features',\n",
    "    'Students_Cleaned_Encoded_selected_20.csv': 'Top 20 features by importance',\n",
    "    'Students_Cleaned_Encoded_statistical.csv': 'Statistically selected features',\n",
    "    'Students_Cleaned_Encoded_pca_ready.csv': 'Numeric features ready for PCA'\n",
    "}\n",
    "\n",
    "print(\"\\n📊 AVAILABLE DATASETS:\")\n",
    "for filename, description in datasets.items():\n",
    "    if os.path.exists(filename):\n",
    "        df_temp = pd.read_csv(filename)\n",
    "        print(f\"✅ {filename}\")\n",
    "        print(f\"   {description}\")\n",
    "        print(f\"   Shape: {df_temp.shape}\")\n",
    "        print(f\"   Features: {df_temp.shape[1] - 1} (+ target)\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"❌ {filename} - Not found\")\n",
    "        print()\n",
    "\n",
    "print(\"🏆 RECOMMENDATIONS BY USE CASE:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n1. 🚀 INITIAL MODEL DEVELOPMENT:\")\n",
    "print(\"   Dataset: Students_Cleaned_Encoded_selected_20.csv\")\n",
    "print(\"   Why: Best performance (R² = 0.5745) with optimal feature count\")\n",
    "print(\"   Use for: Quick prototyping, baseline models, feature importance analysis\")\n",
    "\n",
    "print(\"\\n2. 🎯 PRODUCTION MODELS:\")\n",
    "print(\"   Dataset: Students_Cleaned_Encoded_v1.csv\")\n",
    "print(\"   Why: Balanced performance with well-tested features\")\n",
    "print(\"   Use for: Final models, deployment, consistent results\")\n",
    "\n",
    "print(\"\\n3. 🔬 COMPREHENSIVE ANALYSIS:\")\n",
    "print(\"   Dataset: Students_Cleaned_Encoded_full.csv\")\n",
    "print(\"   Why: All features including advanced engineered features\")\n",
    "print(\"   Use for: Deep analysis, feature exploration, ensemble methods\")\n",
    "\n",
    "print(\"\\n4. ⚡ FAST TRAINING:\")\n",
    "print(\"   Dataset: Students_Cleaned_Encoded_statistical.csv\")\n",
    "print(\"   Why: Statistically selected features for efficiency\")\n",
    "print(\"   Use for: Large-scale experiments, hyperparameter tuning\")\n",
    "\n",
    "print(\"\\n5. 📈 DIMENSIONALITY REDUCTION:\")\n",
    "print(\"   Dataset: Students_Cleaned_Encoded_pca_ready.csv\")\n",
    "print(\"   Why: Prepared for PCA, standardized features\")\n",
    "print(\"   Use for: PCA, t-SNE, clustering, linear models\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎖️  FINAL RECOMMENDATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"For MOST USERS: Start with 'Students_Cleaned_Encoded_selected_20.csv'\")\n",
    "print(\"• Best performance-to-complexity ratio\")\n",
    "print(\"• Fastest training time\")\n",
    "print(\"• Good interpretability\")\n",
    "print(\"• Reduced overfitting risk\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
