{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01a1b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6ffb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03c427ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file\n",
    "data = pd.read_csv('Students.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c17f3fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (3614, 16)\n"
     ]
    }
   ],
   "source": [
    "# data shape\n",
    "print(f\"Data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06821bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUPLICATE ANALYSIS ===\n",
      "Original data duplicates (including all columns): 0\n",
      "Dropped identifying columns: ['Student_Name', 'State', 'College_Name', 'Stream']\n",
      "Duplicates after dropping identifying columns: 580\n",
      "âœ… 580 students have identical responses across all features\n",
      "   This is normal - multiple students can have the same usage patterns\n",
      "\n",
      "ðŸ“Š Sample of identical response patterns:\n",
      "Showing 6 rows with identical patterns...\n",
      "    Year_of_Study  Daily_Usage_Hours  Trust_in_AI_Tools  Impact_on_Grades\n",
      "0               4                0.9                  2                 2\n",
      "4               1                0.9                  1                 3\n",
      "9               2                1.4                  3                -2\n",
      "19              1                4.4                  1                 2\n",
      "22              1                3.4                  1                 0\n",
      "32              2                0.7                  3                 0\n"
     ]
    }
   ],
   "source": [
    "# Check duplicates in original data\n",
    "print(\"=== DUPLICATE ANALYSIS ===\")\n",
    "duplicates_original = data.duplicated().sum()\n",
    "print(f\"Original data duplicates (including all columns): {duplicates_original}\")\n",
    "\n",
    "# Check duplicates after dropping identifying columns\n",
    "identifying_cols = ['Student_Name', 'State', 'College_Name', 'Stream']\n",
    "existing_identifying_cols = [col for col in identifying_cols if col in data.columns]\n",
    "data_without_ids = data.drop(columns=existing_identifying_cols)\n",
    "duplicates_after_drop = data_without_ids.duplicated().sum()\n",
    "\n",
    "print(f\"Dropped identifying columns: {existing_identifying_cols}\")\n",
    "print(f\"Duplicates after dropping identifying columns: {duplicates_after_drop}\")\n",
    "\n",
    "if duplicates_after_drop > 0:\n",
    "    print(f\"âœ… {duplicates_after_drop} students have identical responses across all features\")\n",
    "    print(\"   This is normal - multiple students can have the same usage patterns\")\n",
    "else:\n",
    "    print(\"âœ… No duplicate response patterns found\")\n",
    "\n",
    "# Optional: Show some examples of identical response patterns\n",
    "if duplicates_after_drop > 0:\n",
    "    print(\"\\nðŸ“Š Sample of identical response patterns:\")\n",
    "    duplicate_mask = data_without_ids.duplicated(keep=False)\n",
    "    sample_duplicates = data_without_ids[duplicate_mask].head(6)\n",
    "    print(f\"Showing {len(sample_duplicates)} rows with identical patterns...\")\n",
    "    print(sample_duplicates[['Year_of_Study', 'Daily_Usage_Hours', 'Trust_in_AI_Tools', 'Impact_on_Grades']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d98e94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identifying columns while keeping identical response patterns\n",
    "# Note: We keep rows with identical responses because they represent different students\n",
    "# with the same usage patterns, which is valuable data for modeling\n",
    "data_clean = data.drop(columns=['State', 'Student_Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c94020",
   "metadata": {},
   "source": [
    "# Use Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4ff21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column for Use_Cases split by a comma \n",
    "data_clean['Use_Cases_Split'] = data_clean['Use_Cases'].str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d51e024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All individual use cases and their frequencies:\n",
      "'coding help': 766\n",
      "'assignments': 749\n",
      "'mcq practice': 692\n",
      "'doubt solving': 638\n",
      "'resume writing': 617\n",
      "'content writing': 602\n",
      "'learning new topics': 594\n",
      "'projects': 470\n",
      "'exam prep': 451\n",
      "'notes': 451\n",
      "'project work': 229\n",
      "'exam preparation': 202\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# First, let's extract all individual use cases from the split data\n",
    "def extract_all_individual_use_cases(df, column='Use_Cases_Split'):\n",
    "    \"\"\"\n",
    "    Extract all individual use cases from the split column\n",
    "    \"\"\"\n",
    "    all_individual_cases = []\n",
    "    for case_list in df[column].dropna():\n",
    "        for case in case_list:\n",
    "            if case.strip():  # Only add non-empty cases\n",
    "                all_individual_cases.append(case.strip().lower())\n",
    "    return all_individual_cases\n",
    "\n",
    "# Get all individual use cases\n",
    "all_cases = extract_all_individual_use_cases(data_clean)\n",
    "case_counts = Counter(all_cases)\n",
    "\n",
    "print(\"All individual use cases and their frequencies:\")\n",
    "for case, count in case_counts.most_common():\n",
    "    print(f\"'{case}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3eccd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_individual_use_cases(df):\n",
    "    \"\"\"\n",
    "    Standardize individual use cases using the mapping\n",
    "    \"\"\"\n",
    "    def apply_mapping_to_list(case_list):\n",
    "        # Handle None/NaN cases\n",
    "        if case_list is None or (isinstance(case_list, float) and pd.isna(case_list)):\n",
    "            return case_list\n",
    "        \n",
    "        # Handle empty list\n",
    "        if not isinstance(case_list, list) or len(case_list) == 0:\n",
    "            return case_list\n",
    "        \n",
    "        standardized_cases = []\n",
    "        for case in case_list:\n",
    "            if case and isinstance(case, str):  # Check if case is not None and is string\n",
    "                # Clean and normalize the case\n",
    "                cleaned_case = case.strip().lower()\n",
    "                # Apply mapping\n",
    "                standardized_case = use_case_mapping.get(cleaned_case, cleaned_case)\n",
    "                # Avoid duplicates\n",
    "                if standardized_case not in standardized_cases:\n",
    "                    standardized_cases.append(standardized_case)\n",
    "        \n",
    "        return standardized_cases\n",
    "    \n",
    "    return df['Use_Cases_Split'].apply(apply_mapping_to_list)\n",
    "\n",
    "# Apply standardization\n",
    "data_clean['Use_Cases_Split_Standardized'] = standardize_individual_use_cases(data_clean)\n",
    "\n",
    "# Convert back to string format for easier viewing\n",
    "data_clean['Use_Cases_Standardized'] = data_clean['Use_Cases_Split_Standardized'].apply(\n",
    "    lambda x: ', '.join([case.title() for case in x]) if x and isinstance(x, list) else ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "541d88e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary use case features:\n",
      "uses_coding_help: 766 students\n",
      "uses_assignments: 749 students\n",
      "uses_project_work: 699 students\n",
      "uses_mcq_practice: 692 students\n",
      "uses_exam_preparation: 653 students\n",
      "uses_doubt_solving: 638 students\n",
      "uses_resume_writing: 617 students\n",
      "uses_content_writing: 602 students\n",
      "uses_learning_new_topics: 594 students\n",
      "uses_notes: 451 students\n"
     ]
    }
   ],
   "source": [
    "# Create binary features for each use case\n",
    "def create_binary_use_case_features(df):\n",
    "    \"\"\"\n",
    "    Create binary columns for each unique use case\n",
    "    \"\"\"\n",
    "    # Get all unique use cases from standardized data\n",
    "    all_use_cases = ['coding help', 'assignments', 'project work', 'mcq practice', \n",
    "                     'exam preparation', 'doubt solving', 'resume writing', \n",
    "                     'content writing', 'learning new topics', 'notes']\n",
    "    \n",
    "    # Create binary columns\n",
    "    for use_case in all_use_cases:\n",
    "        col_name = f'uses_{use_case.replace(\" \", \"_\")}'\n",
    "        df[col_name] = df['Use_Cases_Split_Standardized'].apply(\n",
    "            lambda x: 1 if x and use_case in x else 0\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function\n",
    "data_clean = create_binary_use_case_features(data_clean)\n",
    "\n",
    "# Show the new binary features\n",
    "binary_features = [col for col in data_clean.columns if col.startswith('uses_')]\n",
    "print(\"Binary use case features:\")\n",
    "for feature in binary_features:\n",
    "    print(f\"{feature}: {data_clean[feature].sum()} students\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f9b729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5\n",
    "# data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82339c",
   "metadata": {},
   "source": [
    "# AI_Tools_Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d851624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING DUMMY VARIABLES FOR AI TOOLS ===\n",
      "Method 1: Using pandas get_dummies\n",
      "Created 7 dummy variables:\n",
      "  â€¢ ai_tool_bard: 151 students use this tool\n",
      "  â€¢ ai_tool_chatgpt: 1557 students use this tool\n",
      "  â€¢ ai_tool_claude: 171 students use this tool\n",
      "  â€¢ ai_tool_copilot: 1516 students use this tool\n",
      "  â€¢ ai_tool_gemini: 1409 students use this tool\n",
      "  â€¢ ai_tool_midjourney: 371 students use this tool\n",
      "  â€¢ ai_tool_other: 167 students use this tool\n",
      "\n",
      "Dummy variables shape: (3614, 7)\n",
      "Sample of dummy variables:\n",
      "   ai_tool_bard  ai_tool_chatgpt  ai_tool_claude  ai_tool_copilot  \\\n",
      "0             0                0               0                0   \n",
      "1             0                1               0                0   \n",
      "2             0                0               0                1   \n",
      "3             0                0               0                1   \n",
      "4             0                0               0                0   \n",
      "\n",
      "   ai_tool_gemini  ai_tool_midjourney  ai_tool_other  \n",
      "0               1                   0              0  \n",
      "1               0                   0              0  \n",
      "2               0                   0              0  \n",
      "3               0                   0              0  \n",
      "4               1                   0              0  \n",
      "\n",
      "Data types:\n",
      "All columns are integers: True\n"
     ]
    }
   ],
   "source": [
    "def create_ai_tools_dummy_variables(df, column_name='AI_Tools_Used'):\n",
    "    \"\"\"\n",
    "    Create dummy variables for comma-separated AI tools column\n",
    "    \"\"\"\n",
    "    print(\"=== CREATING DUMMY VARIABLES FOR AI TOOLS ===\")\n",
    "    \n",
    "    # Method 1: Using pandas get_dummies with separator\n",
    "    print(\"Method 1: Using pandas get_dummies\")\n",
    "    \n",
    "    # Create dummy variables directly from comma-separated values\n",
    "    # Convert to int to get 1/0 instead of True/False\n",
    "    dummies = df[column_name].str.get_dummies(sep=', ').astype(int)\n",
    "    \n",
    "    # Add prefix to column names for clarity\n",
    "    dummies.columns = [f'ai_tool_{col.lower().replace(\" \", \"_\")}' for col in dummies.columns]\n",
    "    \n",
    "    print(f\"Created {len(dummies.columns)} dummy variables:\")\n",
    "    for col in dummies.columns:\n",
    "        print(f\"  â€¢ {col}: {dummies[col].sum()} students use this tool\")\n",
    "    \n",
    "    # Show the dummy variables\n",
    "    print(f\"\\nDummy variables shape: {dummies.shape}\")\n",
    "    print(f\"Sample of dummy variables:\")\n",
    "    print(dummies.head())\n",
    "    \n",
    "    # Verify data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(f\"All columns are integers: {all(dummies[col].dtype in ['int64', 'int32'] for col in dummies.columns)}\")\n",
    "    \n",
    "    return dummies\n",
    "\n",
    "# Apply the AI Tools dummy variables function to data_clean\n",
    "ai_tools_dummies_clean = create_ai_tools_dummy_variables(data_clean, 'AI_Tools_Used') \n",
    "# Apply to your data\n",
    "data_clean_v1 = pd.concat([data_clean, ai_tools_dummies_clean], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e919ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 of data clean v1\n",
    "# data_clean_v1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42f342",
   "metadata": {},
   "source": [
    "## Drop Necessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fffafb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['College_Name', 'Stream', 'Year_of_Study', 'AI_Tools_Used',\n",
       "       'Daily_Usage_Hours', 'Use_Cases', 'Trust_in_AI_Tools',\n",
       "       'Impact_on_Grades', 'Do_Professors_Allow_Use', 'Preferred_AI_Tool',\n",
       "       'Awareness_Level', 'Willing_to_Pay_for_Access', 'Device_Used',\n",
       "       'Internet_Access', 'Use_Cases_Split', 'Use_Cases_Split_Standardized',\n",
       "       'Use_Cases_Standardized', 'uses_coding_help', 'uses_assignments',\n",
       "       'uses_project_work', 'uses_mcq_practice', 'uses_exam_preparation',\n",
       "       'uses_doubt_solving', 'uses_resume_writing', 'uses_content_writing',\n",
       "       'uses_learning_new_topics', 'uses_notes', 'ai_tool_bard',\n",
       "       'ai_tool_chatgpt', 'ai_tool_claude', 'ai_tool_copilot',\n",
       "       'ai_tool_gemini', 'ai_tool_midjourney', 'ai_tool_other'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names of data clean \n",
    "data_clean_v1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9e4c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'Student_Name', 'College_Name', 'Use_Cases', 'State', 'Use_Cases_Advanced', 'Use_Cases_Split','Use_Cases_Length', 'Use_Cases_Split_Standardized'\n",
    "data_cleaned_final = data_clean_v1.drop(columns=['College_Name', 'Stream', 'AI_Tools_Used', 'Use_Cases', 'Use_Cases_Split', 'Use_Cases_Split_Standardized', 'Use_Cases_Standardized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ceb16ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 of data cleaned final\n",
    "# data_cleaned_final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ae383",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6c9ac2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded Preferred_AI_Tool: 6 features created\n",
      "One-hot encoded Device_Used: 3 features created\n",
      "One-hot encoded Internet_Access: 3 features created\n",
      "Binary encoded Do_Professors_Allow_Use: 0/1\n",
      "Binary encoded Willing_to_Pay_for_Access: 0/1\n",
      "\n",
      "Skipping Trust_in_AI_Tools and Impact_on_Grades - already numerical\n",
      "Trust_in_AI_Tools type: int64\n",
      "Impact_on_Grades type: int64\n",
      "\n",
      "New encoded columns created:\n",
      "['preferred_ai_tool_Bard', 'preferred_ai_tool_ChatGPT', 'preferred_ai_tool_Claude', 'preferred_ai_tool_Copilot', 'preferred_ai_tool_Gemini', 'preferred_ai_tool_Other', 'device_used_Laptop', 'device_used_Mobile', 'device_used_Tablet', 'internet_access_High', 'internet_access_Medium', 'internet_access_Poor', 'Do_Professors_Allow_Use_encoded', 'Willing_to_Pay_for_Access_encoded']\n",
      "\n",
      "Sample of encoded data:\n",
      "   Trust_in_AI_Tools  Impact_on_Grades  preferred_ai_tool_Bard  \\\n",
      "0                  2                 2                       0   \n",
      "1                  3                -3                       0   \n",
      "2                  5                 0                       0   \n",
      "3                  5                 2                       0   \n",
      "4                  1                 3                       0   \n",
      "\n",
      "   preferred_ai_tool_ChatGPT  preferred_ai_tool_Claude  \\\n",
      "0                          0                         0   \n",
      "1                          0                         0   \n",
      "2                          0                         0   \n",
      "3                          0                         0   \n",
      "4                          0                         0   \n",
      "\n",
      "   preferred_ai_tool_Copilot  preferred_ai_tool_Gemini  \n",
      "0                          1                         0  \n",
      "1                          0                         0  \n",
      "2                          0                         1  \n",
      "3                          0                         1  \n",
      "4                          0                         0  \n",
      "\n",
      "Data types of encoded columns:\n",
      "preferred_ai_tool_Bard: int32 | Unique values: [0, 1]\n",
      "preferred_ai_tool_ChatGPT: int32 | Unique values: [0, 1]\n",
      "preferred_ai_tool_Claude: int32 | Unique values: [0, 1]\n",
      "preferred_ai_tool_Copilot: int32 | Unique values: [0, 1]\n",
      "preferred_ai_tool_Gemini: int32 | Unique values: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "def apply_appropriate_encoding(df):\n",
    "    \"\"\"\n",
    "    Apply the correct encoding strategy for each categorical column\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. ONE-HOT ENCODING for nominal categories\n",
    "    nominal_columns = ['AI_Tools_Used', 'Preferred_AI_Tool', 'Device_Used', 'Internet_Access']\n",
    "    \n",
    "    for col in nominal_columns:\n",
    "        if col in df.columns:\n",
    "            # Create dummy variables with dtype=int to get 1/0 instead of True/False\n",
    "            dummies = pd.get_dummies(df[col], prefix=col.lower(), drop_first=False, dtype=int)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "            print(f\"One-hot encoded {col}: {len(dummies.columns)} features created\")\n",
    "    \n",
    "    # 2. BINARY ENCODING for yes/no columns only\n",
    "    binary_mappings = {\n",
    "        'Do_Professors_Allow_Use': {'No': 0, 'Yes': 1},\n",
    "        'Willing_to_Pay_for_Access': {'No': 0, 'Yes': 1}\n",
    "    }\n",
    "    \n",
    "    for col, mapping in binary_mappings.items():\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_encoded'] = df[col].map(mapping)\n",
    "            print(f\"Binary encoded {col}: 0/1\")\n",
    "    \n",
    "    # 3. Skip Trust_in_AI_Tools and Impact_on_Grades if they're already numerical\n",
    "    print(f\"\\nSkipping Trust_in_AI_Tools and Impact_on_Grades - already numerical\")\n",
    "    print(f\"Trust_in_AI_Tools type: {df['Trust_in_AI_Tools'].dtype}\")\n",
    "    print(f\"Impact_on_Grades type: {df['Impact_on_Grades'].dtype}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply encoding strategy\n",
    "data_encoded = apply_appropriate_encoding(data_cleaned_final)\n",
    "\n",
    "# Show the results\n",
    "print(\"\\nNew encoded columns created:\")\n",
    "encoded_cols = [col for col in data_encoded.columns if any(x in col for x in ['_encoded', 'stream_', 'ai_tools_used_', 'preferred_ai_tool_', 'device_used_', 'internet_access_'])]\n",
    "print(encoded_cols)\n",
    "\n",
    "# Display sample of encoded data\n",
    "print(\"\\nSample of encoded data:\")\n",
    "print(data_encoded[['Trust_in_AI_Tools', 'Impact_on_Grades'] + encoded_cols[:5]].head())\n",
    "\n",
    "# Verify data types of encoded columns\n",
    "print(\"\\nData types of encoded columns:\")\n",
    "for col in encoded_cols[:5]:\n",
    "    if col in data_encoded.columns:\n",
    "        print(f\"{col}: {data_encoded[col].dtype} | Unique values: {sorted(data_encoded[col].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e5591",
   "metadata": {},
   "source": [
    "## Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d77ca2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "data_encoded_clean = data_encoded.drop(columns=[\n",
    "    'Do_Professors_Allow_Use', 'Preferred_AI_Tool',\n",
    "    'Willing_to_Pay_for_Access', 'Device_Used',\n",
    "       'Internet_Access'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "09638745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final data shape after encoding: (3614, 36)\n"
     ]
    }
   ],
   "source": [
    "# data shape of encoded clean data\n",
    "print(f\"\\nFinal data shape after encoding: {data_encoded_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9150cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as a csv file\n",
    "data_encoded_clean.to_csv('Students_Cleaned_Encoded_v1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
