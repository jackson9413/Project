{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a0dc6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd7740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Students_Cleaned_Encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ec0cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Study</th>\n",
       "      <th>Daily_Usage_Hours</th>\n",
       "      <th>Trust_in_AI_Tools</th>\n",
       "      <th>Impact_on_Grades</th>\n",
       "      <th>Awareness_Level</th>\n",
       "      <th>uses_coding_help</th>\n",
       "      <th>uses_assignments</th>\n",
       "      <th>uses_project_work</th>\n",
       "      <th>uses_mcq_practice</th>\n",
       "      <th>uses_exam_preparation</th>\n",
       "      <th>...</th>\n",
       "      <th>preferred_ai_tool_Gemini</th>\n",
       "      <th>preferred_ai_tool_Other</th>\n",
       "      <th>device_used_Laptop</th>\n",
       "      <th>device_used_Mobile</th>\n",
       "      <th>device_used_Tablet</th>\n",
       "      <th>internet_access_High</th>\n",
       "      <th>internet_access_Medium</th>\n",
       "      <th>internet_access_Poor</th>\n",
       "      <th>Do_Professors_Allow_Use_encoded</th>\n",
       "      <th>Willing_to_Pay_for_Access_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Study  Daily_Usage_Hours  Trust_in_AI_Tools  Impact_on_Grades  \\\n",
       "0              4                0.9                  2                 2   \n",
       "1              2                3.4                  3                -3   \n",
       "2              2                3.6                  5                 0   \n",
       "3              2                2.9                  5                 2   \n",
       "4              1                0.9                  1                 3   \n",
       "\n",
       "   Awareness_Level  uses_coding_help  uses_assignments  uses_project_work  \\\n",
       "0                9                 1                 1                  0   \n",
       "1                6                 0                 0                  0   \n",
       "2                1                 0                 0                  1   \n",
       "3                5                 0                 0                  0   \n",
       "4                8                 0                 0                  0   \n",
       "\n",
       "   uses_mcq_practice  uses_exam_preparation  ...  preferred_ai_tool_Gemini  \\\n",
       "0                  0                      0  ...                         0   \n",
       "1                  0                      0  ...                         0   \n",
       "2                  1                      0  ...                         1   \n",
       "3                  0                      0  ...                         1   \n",
       "4                  0                      0  ...                         0   \n",
       "\n",
       "   preferred_ai_tool_Other  device_used_Laptop  device_used_Mobile  \\\n",
       "0                        0                   0                   1   \n",
       "1                        1                   1                   0   \n",
       "2                        0                   0                   0   \n",
       "3                        0                   1                   0   \n",
       "4                        1                   1                   0   \n",
       "\n",
       "   device_used_Tablet  internet_access_High  internet_access_Medium  \\\n",
       "0                   0                     0                       0   \n",
       "1                   0                     0                       0   \n",
       "2                   1                     0                       0   \n",
       "3                   0                     1                       0   \n",
       "4                   0                     0                       1   \n",
       "\n",
       "   internet_access_Poor  Do_Professors_Allow_Use_encoded  \\\n",
       "0                     1                                0   \n",
       "1                     1                                1   \n",
       "2                     1                                0   \n",
       "3                     0                                1   \n",
       "4                     0                                1   \n",
       "\n",
       "   Willing_to_Pay_for_Access_encoded  \n",
       "0                                  1  \n",
       "1                                  0  \n",
       "2                                  0  \n",
       "3                                  0  \n",
       "4                                  1  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 5\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bea634c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE REDUNDANCY ANALYSIS ===\n",
      "AI Tool Usage Features: 7\n",
      "Preferred Tool Features: 6\n",
      "ChatGPT usage vs preference correlation: 0.020\n",
      "\n",
      "Highly correlated features (>0.8):\n",
      "  uses_exam_preparation ↔ uses_notes: 0.804\n"
     ]
    }
   ],
   "source": [
    "# 1. Check for redundant/duplicate features\n",
    "def analyze_feature_redundancy(data):\n",
    "    \"\"\"\n",
    "    Identify potentially redundant features\n",
    "    \"\"\"\n",
    "    print(\"=== FEATURE REDUNDANCY ANALYSIS ===\")\n",
    "    \n",
    "    # Check for duplicate AI tool features\n",
    "    ai_tool_features = [col for col in data.columns if col.startswith('ai_tool_')]\n",
    "    preferred_tool_features = [col for col in data.columns if col.startswith('preferred_ai_tool_')]\n",
    "    \n",
    "    print(f\"AI Tool Usage Features: {len(ai_tool_features)}\")\n",
    "    print(f\"Preferred Tool Features: {len(preferred_tool_features)}\")\n",
    "    \n",
    "    # Check correlation between similar features\n",
    "    if 'ai_tool_chatgpt' in data.columns and 'preferred_ai_tool_ChatGPT' in data.columns:\n",
    "        corr = data['ai_tool_chatgpt'].corr(data['preferred_ai_tool_ChatGPT'])\n",
    "        print(f\"ChatGPT usage vs preference correlation: {corr:.3f}\")\n",
    "    \n",
    "    # Look for highly correlated features\n",
    "    corr_matrix = data.corr()\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "                high_corr_pairs.append((\n",
    "                    corr_matrix.columns[i], \n",
    "                    corr_matrix.columns[j], \n",
    "                    corr_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    print(f\"\\nHighly correlated features (>0.8):\")\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"  {feat1} ↔ {feat2}: {corr:.3f}\")\n",
    "    \n",
    "    return high_corr_pairs\n",
    "\n",
    "# Run redundancy analysis\n",
    "redundant_features = analyze_feature_redundancy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40d238ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADVANCED FEATURE ENGINEERING ===\n",
      "Created 9 new features\n",
      "New features: ['ai_tool_diversity', 'use_case_diversity', 'tech_adoption_score', 'academic_focus', 'professional_focus', 'device_quality', 'internet_quality', 'trust_x_usage', 'year_x_usage']\n"
     ]
    }
   ],
   "source": [
    "# 2. Create more meaningful feature engineering\n",
    "def create_advanced_features(data):\n",
    "    \"\"\"\n",
    "    Create more sophisticated features\n",
    "    \"\"\"\n",
    "    print(\"=== ADVANCED FEATURE ENGINEERING ===\")\n",
    "    \n",
    "    data_enhanced = data.copy()\n",
    "    \n",
    "    # 1. AI Tool Diversity Score\n",
    "    ai_tool_cols = [col for col in data.columns if col.startswith('ai_tool_')]\n",
    "    data_enhanced['ai_tool_diversity'] = data[ai_tool_cols].sum(axis=1)\n",
    "    \n",
    "    # 2. Use Case Diversity Score\n",
    "    use_case_cols = [col for col in data.columns if col.startswith('uses_')]\n",
    "    data_enhanced['use_case_diversity'] = data[use_case_cols].sum(axis=1)\n",
    "    \n",
    "    # 3. Technology Adoption Score (composite)\n",
    "    tech_features = ['Trust_in_AI_Tools', 'Awareness_Level', 'ai_tool_diversity']\n",
    "    data_enhanced['tech_adoption_score'] = data_enhanced[tech_features].sum(axis=1)\n",
    "    \n",
    "    # 4. Academic Focus Score\n",
    "    academic_uses = ['uses_assignments', 'uses_exam_preparation', 'uses_project_work']\n",
    "    data_enhanced['academic_focus'] = data_enhanced[academic_uses].sum(axis=1)\n",
    "    \n",
    "    # 5. Professional Focus Score\n",
    "    professional_uses = ['uses_resume_writing', 'uses_content_writing', 'uses_coding_help']\n",
    "    data_enhanced['professional_focus'] = data_enhanced[professional_uses].sum(axis=1)\n",
    "    \n",
    "    # 6. Device Quality Score\n",
    "    if 'device_used_Laptop' in data.columns:\n",
    "        data_enhanced['device_quality'] = (\n",
    "            data_enhanced['device_used_Laptop'] * 3 +\n",
    "            data_enhanced['device_used_Tablet'] * 2 +\n",
    "            data_enhanced['device_used_Mobile'] * 1\n",
    "        )\n",
    "    \n",
    "    # 7. Internet Quality Score\n",
    "    if 'internet_access_High' in data.columns:\n",
    "        data_enhanced['internet_quality'] = (\n",
    "            data_enhanced['internet_access_High'] * 3 +\n",
    "            data_enhanced['internet_access_Medium'] * 2 +\n",
    "            data_enhanced['internet_access_Poor'] * 1\n",
    "        )\n",
    "    \n",
    "    # 8. Interaction terms\n",
    "    data_enhanced['trust_x_usage'] = data_enhanced['Trust_in_AI_Tools'] * data_enhanced['Daily_Usage_Hours']\n",
    "    data_enhanced['year_x_usage'] = data_enhanced['Year_of_Study'] * data_enhanced['Daily_Usage_Hours']\n",
    "    \n",
    "    print(f\"Created {len(data_enhanced.columns) - len(data.columns)} new features\")\n",
    "    \n",
    "    new_features = [col for col in data_enhanced.columns if col not in data.columns]\n",
    "    print(f\"New features: {new_features}\")\n",
    "    \n",
    "    return data_enhanced, new_features\n",
    "\n",
    "# Apply advanced feature engineering\n",
    "data_enhanced, new_features = create_advanced_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c02e320d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Study</th>\n",
       "      <th>Daily_Usage_Hours</th>\n",
       "      <th>Trust_in_AI_Tools</th>\n",
       "      <th>Impact_on_Grades</th>\n",
       "      <th>Awareness_Level</th>\n",
       "      <th>uses_coding_help</th>\n",
       "      <th>uses_assignments</th>\n",
       "      <th>uses_project_work</th>\n",
       "      <th>uses_mcq_practice</th>\n",
       "      <th>uses_exam_preparation</th>\n",
       "      <th>...</th>\n",
       "      <th>Willing_to_Pay_for_Access_encoded</th>\n",
       "      <th>ai_tool_diversity</th>\n",
       "      <th>use_case_diversity</th>\n",
       "      <th>tech_adoption_score</th>\n",
       "      <th>academic_focus</th>\n",
       "      <th>professional_focus</th>\n",
       "      <th>device_quality</th>\n",
       "      <th>internet_quality</th>\n",
       "      <th>trust_x_usage</th>\n",
       "      <th>year_x_usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>14.5</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Study  Daily_Usage_Hours  Trust_in_AI_Tools  Impact_on_Grades  \\\n",
       "0              4                0.9                  2                 2   \n",
       "1              2                3.4                  3                -3   \n",
       "2              2                3.6                  5                 0   \n",
       "3              2                2.9                  5                 2   \n",
       "4              1                0.9                  1                 3   \n",
       "\n",
       "   Awareness_Level  uses_coding_help  uses_assignments  uses_project_work  \\\n",
       "0                9                 1                 1                  0   \n",
       "1                6                 0                 0                  0   \n",
       "2                1                 0                 0                  1   \n",
       "3                5                 0                 0                  0   \n",
       "4                8                 0                 0                  0   \n",
       "\n",
       "   uses_mcq_practice  uses_exam_preparation  ...  \\\n",
       "0                  0                      0  ...   \n",
       "1                  0                      0  ...   \n",
       "2                  1                      0  ...   \n",
       "3                  0                      0  ...   \n",
       "4                  0                      0  ...   \n",
       "\n",
       "   Willing_to_Pay_for_Access_encoded  ai_tool_diversity  use_case_diversity  \\\n",
       "0                                  1                  1                   2   \n",
       "1                                  0                  1                   1   \n",
       "2                                  0                  1                   2   \n",
       "3                                  0                  1                   1   \n",
       "4                                  1                  1                   2   \n",
       "\n",
       "   tech_adoption_score  academic_focus  professional_focus  device_quality  \\\n",
       "0                   12               1                   1               1   \n",
       "1                   10               0                   0               3   \n",
       "2                    7               1                   0               2   \n",
       "3                   11               0                   1               3   \n",
       "4                   10               0                   1               3   \n",
       "\n",
       "   internet_quality  trust_x_usage  year_x_usage  \n",
       "0                 1            1.8           3.6  \n",
       "1                 1           10.2           6.8  \n",
       "2                 1           18.0           7.2  \n",
       "3                 3           14.5           5.8  \n",
       "4                 2            0.9           0.9  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 5\n",
    "data_enhanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b916e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year_of_Study',\n",
       " 'Daily_Usage_Hours',\n",
       " 'Trust_in_AI_Tools',\n",
       " 'Impact_on_Grades',\n",
       " 'Awareness_Level',\n",
       " 'uses_coding_help',\n",
       " 'uses_assignments',\n",
       " 'uses_project_work',\n",
       " 'uses_mcq_practice',\n",
       " 'uses_exam_preparation',\n",
       " 'uses_doubt_solving',\n",
       " 'uses_resume_writing',\n",
       " 'uses_content_writing',\n",
       " 'uses_learning_new_topics',\n",
       " 'uses_notes',\n",
       " 'ai_tool_bard',\n",
       " 'ai_tool_chatgpt',\n",
       " 'ai_tool_claude',\n",
       " 'ai_tool_copilot',\n",
       " 'ai_tool_gemini',\n",
       " 'ai_tool_midjourney',\n",
       " 'ai_tool_other',\n",
       " 'preferred_ai_tool_Bard',\n",
       " 'preferred_ai_tool_ChatGPT',\n",
       " 'preferred_ai_tool_Claude',\n",
       " 'preferred_ai_tool_Copilot',\n",
       " 'preferred_ai_tool_Gemini',\n",
       " 'preferred_ai_tool_Other',\n",
       " 'device_used_Laptop',\n",
       " 'device_used_Mobile',\n",
       " 'device_used_Tablet',\n",
       " 'internet_access_High',\n",
       " 'internet_access_Medium',\n",
       " 'internet_access_Poor',\n",
       " 'Do_Professors_Allow_Use_encoded',\n",
       " 'Willing_to_Pay_for_Access_encoded',\n",
       " 'ai_tool_diversity',\n",
       " 'use_case_diversity',\n",
       " 'tech_adoption_score',\n",
       " 'academic_focus',\n",
       " 'professional_focus',\n",
       " 'device_quality',\n",
       " 'internet_quality',\n",
       " 'trust_x_usage',\n",
       " 'year_x_usage']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names\n",
    "data_enhanced.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dfe8a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA QUALITY VALIDATION ===\n",
      "✅ No missing values\n",
      "✅ No constant features\n",
      "\n",
      "Feature Distribution Summary:\n",
      "Binary features: 31\n",
      "Categorical features: 3\n",
      "Continuous features: 2\n",
      "✅ No extreme multicollinearity\n"
     ]
    }
   ],
   "source": [
    "# 3. Comprehensive data validation\n",
    "def validate_data_quality(data):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality check\n",
    "    \"\"\"\n",
    "    print(\"=== DATA QUALITY VALIDATION ===\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_counts = data.isnull().sum()\n",
    "    if missing_counts.sum() > 0:\n",
    "        print(f\"⚠️ Missing values found:\")\n",
    "        print(missing_counts[missing_counts > 0])\n",
    "    else:\n",
    "        print(\"✅ No missing values\")\n",
    "    \n",
    "    # Check for constant features\n",
    "    constant_features = []\n",
    "    for col in data.columns:\n",
    "        if data[col].nunique() <= 1:\n",
    "            constant_features.append(col)\n",
    "    \n",
    "    if constant_features:\n",
    "        print(f\"⚠️ Constant features (remove these): {constant_features}\")\n",
    "    else:\n",
    "        print(\"✅ No constant features\")\n",
    "    \n",
    "    # Check feature distributions\n",
    "    print(f\"\\nFeature Distribution Summary:\")\n",
    "    print(f\"Binary features: {sum(data[col].nunique() == 2 for col in data.columns)}\")\n",
    "    print(f\"Categorical features: {sum(2 < data[col].nunique() <= 10 for col in data.columns)}\")\n",
    "    print(f\"Continuous features: {sum(data[col].nunique() > 10 for col in data.columns)}\")\n",
    "    \n",
    "    # Check for multicollinearity\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    corr_matrix = data[numeric_cols].corr()\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    high_corr_count = 0\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "                high_corr_count += 1\n",
    "    \n",
    "    if high_corr_count > 0:\n",
    "        print(f\"⚠️ {high_corr_count} highly correlated pairs (>0.9)\")\n",
    "    else:\n",
    "        print(\"✅ No extreme multicollinearity\")\n",
    "    \n",
    "    return constant_features\n",
    "\n",
    "# Validate data quality\n",
    "issues = validate_data_quality(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee6b8a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADVANCED MODELING PIPELINE ===\n",
      "Selected 20 features using statistical selection\n",
      "\n",
      "Optimizing Random Forest...\n",
      "  Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "  CV R²: 0.8780 (±0.0238)\n",
      "\n",
      "Optimizing Gradient Boosting...\n",
      "  Best parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "  CV R²: 0.8593 (±0.0156)\n",
      "\n",
      "Optimizing ElasticNet...\n",
      "  Best parameters: {'alpha': 0.1, 'l1_ratio': 0.1}\n",
      "  CV R²: 0.6348 (±0.0241)\n"
     ]
    }
   ],
   "source": [
    "# 4. Advanced modeling approach\n",
    "def advanced_modeling_pipeline(data, target_col='Daily_Usage_Hours'):\n",
    "    \"\"\"\n",
    "    Comprehensive modeling pipeline\n",
    "    \"\"\"\n",
    "    print(\"=== ADVANCED MODELING PIPELINE ===\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # 1. Feature Selection\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    # Statistical feature selection\n",
    "    selector = SelectKBest(score_func=f_regression, k=20)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features using statistical selection\")\n",
    "    \n",
    "    # 2. Model with hyperparameter tuning\n",
    "    from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    \n",
    "    models = {\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'learning_rate': [0.1, 0.05],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        },\n",
    "        'ElasticNet': {\n",
    "            'model': ElasticNet(random_state=42),\n",
    "            'params': {\n",
    "                'alpha': [0.1, 1.0, 10.0],\n",
    "                'l1_ratio': [0.1, 0.5, 0.9]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Grid search for best models\n",
    "    best_models = {}\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X[selected_features], y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    for name, model_info in models.items():\n",
    "        print(f\"\\nOptimizing {name}...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            model_info['model'], \n",
    "            model_info['params'],\n",
    "            cv=5,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='r2')\n",
    "        \n",
    "        print(f\"  Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"  CV R²: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "        \n",
    "        best_models[name] = {\n",
    "            'model': best_model,\n",
    "            'cv_score': cv_scores.mean(),\n",
    "            'params': grid_search.best_params_\n",
    "        }\n",
    "    \n",
    "    return best_models, selected_features\n",
    "\n",
    "# Run advanced modeling\n",
    "best_models, top_features = advanced_modeling_pipeline(data_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1a98815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Impact_on_Grades', 'uses_project_work', 'uses_mcq_practice',\n",
       "       'uses_exam_preparation', 'uses_doubt_solving', 'uses_resume_writing',\n",
       "       'uses_learning_new_topics', 'ai_tool_chatgpt', 'ai_tool_copilot',\n",
       "       'ai_tool_other', 'preferred_ai_tool_Bard', 'preferred_ai_tool_ChatGPT',\n",
       "       'preferred_ai_tool_Claude', 'preferred_ai_tool_Other',\n",
       "       'Willing_to_Pay_for_Access_encoded', 'ai_tool_diversity',\n",
       "       'use_case_diversity', 'professional_focus', 'trust_x_usage',\n",
       "       'year_x_usage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b37cb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = data_enhanced.drop(columns=[\"Daily_Usage_Hours\"])\n",
    "y = data_enhanced[\"Daily_Usage_Hours\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88d7c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f386967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(score_func=f_regression, k=20)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb24d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X[selected_features], y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75ff8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c71c1bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL INTERPRETABILITY ===\n",
      "Top 10 Most Important Features:\n",
      " 1. year_x_usage                   0.6162\n",
      " 2. trust_x_usage                  0.2748\n",
      " 3. Impact_on_Grades               0.0252\n",
      " 4. ai_tool_diversity              0.0108\n",
      " 5. use_case_diversity             0.0103\n",
      " 6. preferred_ai_tool_ChatGPT      0.0071\n",
      " 7. professional_focus             0.0061\n",
      " 8. uses_learning_new_topics       0.0050\n",
      " 9. uses_mcq_practice              0.0049\n",
      "10. ai_tool_copilot                0.0048\n",
      "\n",
      "Model Performance:\n",
      "Mean Absolute Error: 0.2159\n",
      "R² Score: 0.8850\n",
      "Predictions within 1 std: 82.3%\n",
      "Predictions within 2 std: 93.2%\n"
     ]
    }
   ],
   "source": [
    "# 5. Model interpretability analysis\n",
    "def model_interpretability(model, feature_names, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Analyze model interpretability\n",
    "    \"\"\"\n",
    "    print(\"=== MODEL INTERPRETABILITY ===\")\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"Top 10 Most Important Features:\")\n",
    "        for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):\n",
    "            print(f\"{i:2d}. {row['feature']:<30} {row['importance']:.4f}\")\n",
    "    \n",
    "    # Model performance analysis\n",
    "    predictions = model.predict(X_test)\n",
    "    residuals = y_test - predictions\n",
    "    \n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"Mean Absolute Error: {np.mean(np.abs(residuals)):.4f}\")\n",
    "    print(f\"R² Score: {r2_score(y_test, predictions):.4f}\")\n",
    "    \n",
    "    # Prediction intervals\n",
    "    within_1_std = np.sum(np.abs(residuals) <= np.std(residuals)) / len(residuals)\n",
    "    within_2_std = np.sum(np.abs(residuals) <= 2 * np.std(residuals)) / len(residuals)\n",
    "    \n",
    "    print(f\"Predictions within 1 std: {within_1_std:.1%}\")\n",
    "    print(f\"Predictions within 2 std: {within_2_std:.1%}\")\n",
    "    \n",
    "    return importance_df if hasattr(model, 'feature_importances_') else None\n",
    "\n",
    "# Apply interpretability analysis\n",
    "if best_models:\n",
    "    best_model_name = max(best_models.keys(), key=lambda x: best_models[x]['cv_score'])\n",
    "    interpretability = model_interpretability(\n",
    "        best_models[best_model_name]['model'], \n",
    "        top_features, \n",
    "        X_test, \n",
    "        y_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5f0a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def correlation_based_selection(data, target_col='Daily_Usage_Hours', min_target_corr=0.05, max_feature_corr=0.8):\n",
    "#     \"\"\"\n",
    "#     Select features based on correlation with target and between features\n",
    "#     \"\"\"\n",
    "#     print(f\"=== CORRELATION-BASED FEATURE SELECTION ===\")\n",
    "#     print(f\"Min target correlation: {min_target_corr}\")\n",
    "#     print(f\"Max inter-feature correlation: {max_feature_corr}\")\n",
    "    \n",
    "#     X = data.drop(columns=[target_col])\n",
    "#     y = data[target_col]\n",
    "    \n",
    "#     # Step 1: Filter by correlation with target\n",
    "#     target_correlations = X.corrwith(y).abs()\n",
    "#     high_target_corr_features = target_correlations[target_correlations >= min_target_corr].index.tolist()\n",
    "    \n",
    "#     print(f\"\\nStep 1: Features with target correlation >= {min_target_corr}\")\n",
    "#     print(f\"Selected {len(high_target_corr_features)} out of {len(X.columns)} features\")\n",
    "    \n",
    "#     if len(high_target_corr_features) == 0:\n",
    "#         print(\"No features meet target correlation threshold!\")\n",
    "#         return []\n",
    "    \n",
    "#     # Step 2: Remove highly correlated features among selected ones\n",
    "#     X_filtered = X[high_target_corr_features]\n",
    "#     corr_matrix = X_filtered.corr()\n",
    "    \n",
    "#     # Find pairs of highly correlated features\n",
    "#     to_remove = set()\n",
    "#     for i in range(len(corr_matrix.columns)):\n",
    "#         for j in range(i+1, len(corr_matrix.columns)):\n",
    "#             if abs(corr_matrix.iloc[i, j]) > max_feature_corr:\n",
    "#                 feat1, feat2 = corr_matrix.columns[i], corr_matrix.columns[j]\n",
    "                \n",
    "#                 # Keep the one with higher target correlation\n",
    "#                 if target_correlations[feat1] >= target_correlations[feat2]:\n",
    "#                     to_remove.add(feat2)\n",
    "#                 else:\n",
    "#                     to_remove.add(feat1)\n",
    "    \n",
    "#     final_features = [f for f in high_target_corr_features if f not in to_remove]\n",
    "    \n",
    "#     print(f\"\\nStep 2: Remove inter-correlated features (>{max_feature_corr})\")\n",
    "#     print(f\"Removed {len(to_remove)} features: {list(to_remove)}\")\n",
    "#     print(f\"Final selection: {len(final_features)} features\")\n",
    "    \n",
    "#     # Show final selected features with their target correlations\n",
    "#     print(f\"\\nFinal Selected Features:\")\n",
    "#     final_target_corrs = target_correlations[final_features].sort_values(ascending=False)\n",
    "#     for i, (feature, corr) in enumerate(final_target_corrs.items(), 1):\n",
    "#         print(f\"{i:2d}. {feature:<35} {corr:.4f}\")\n",
    "    \n",
    "#     return final_features\n",
    "\n",
    "# # Apply correlation-based selection\n",
    "# corr_selected_features = correlation_based_selection(data_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33785c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_final_feature_set(data, statistical_features, correlation_features, target_col='Daily_Usage_Hours'):\n",
    "#     \"\"\"\n",
    "#     Combine different feature selection methods to create final dataset\n",
    "#     \"\"\"\n",
    "#     print(\"=== CREATING FINAL FEATURE SET ===\")\n",
    "    \n",
    "#     # Take union of both methods\n",
    "#     all_selected = list(set(statistical_features + correlation_features))\n",
    "    \n",
    "#     # Always include target variable\n",
    "#     final_features = all_selected + [target_col]\n",
    "    \n",
    "#     # Create final dataset\n",
    "#     data_final = data[final_features].copy()\n",
    "    \n",
    "#     print(f\"Features from statistical selection: {len(statistical_features)}\")\n",
    "#     print(f\"Features from correlation selection: {len(correlation_features)}\")\n",
    "#     print(f\"Overlap between methods: {len(set(statistical_features) & set(correlation_features))}\")\n",
    "#     print(f\"Final feature set size: {len(all_selected)} features + target\")\n",
    "    \n",
    "#     print(f\"\\nFinal Selected Features:\")\n",
    "#     for i, feature in enumerate(sorted(all_selected), 1):\n",
    "#         in_stat = \"S\" if feature in statistical_features else \" \"\n",
    "#         in_corr = \"C\" if feature in correlation_features else \" \"\n",
    "#         print(f\"{i:2d}. [{in_stat}{in_corr}] {feature}\")\n",
    "    \n",
    "#     print(f\"\\nLegend: [S] = Statistical selection, [C] = Correlation selection\")\n",
    "    \n",
    "#     # Final correlation check\n",
    "#     final_corr_matrix = data_final.drop(columns=[target_col]).corr()\n",
    "#     high_corr_remaining = 0\n",
    "#     for i in range(len(final_corr_matrix.columns)):\n",
    "#         for j in range(i+1, len(final_corr_matrix.columns)):\n",
    "#             if abs(final_corr_matrix.iloc[i, j]) > 0.8:\n",
    "#                 high_corr_remaining += 1\n",
    "    \n",
    "#     print(f\"\\nFinal validation:\")\n",
    "#     print(f\"Remaining high correlations (>0.8): {high_corr_remaining}\")\n",
    "#     print(f\"Original dataset: {data.shape[1]} features\")\n",
    "#     print(f\"Final dataset: {data_final.shape[1]} features\")\n",
    "#     print(f\"Reduction: {((data.shape[1] - data_final.shape[1]) / data.shape[1] * 100):.1f}%\")\n",
    "    \n",
    "#     return data_final, all_selected\n",
    "\n",
    "# # Create final dataset\n",
    "# data_final_selected, final_feature_list = create_final_feature_set(\n",
    "#     data_reduced, selected_features, corr_selected_features\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81c35449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5\n",
    "# data_final_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "722567a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f23dbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_feature_selection_results(original_data, final_data, target_col='Daily_Usage_Hours'):\n",
    "#     \"\"\"\n",
    "#     Create visualizations to show the impact of feature selection\n",
    "#     \"\"\"\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "#     # 1. Feature count comparison\n",
    "#     original_features = original_data.shape[1] - 1  # exclude target\n",
    "#     final_features = final_data.shape[1] - 1  # exclude target\n",
    "    \n",
    "#     axes[0,0].bar(['Original', 'Final'], [original_features, final_features], \n",
    "#                   color=['lightcoral', 'lightgreen'])\n",
    "#     axes[0,0].set_title('Feature Count: Before vs After Selection')\n",
    "#     axes[0,0].set_ylabel('Number of Features')\n",
    "#     for i, v in enumerate([original_features, final_features]):\n",
    "#         axes[0,0].text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "#     # 2. Correlation heatmap of final features\n",
    "#     final_corr = final_data.drop(columns=[target_col]).corr()\n",
    "#     im = axes[0,1].imshow(final_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "#     axes[0,1].set_title('Correlation Matrix: Final Selected Features')\n",
    "#     axes[0,1].set_xticks(range(len(final_corr.columns)))\n",
    "#     axes[0,1].set_yticks(range(len(final_corr.columns)))\n",
    "#     axes[0,1].set_xticklabels([f[:10] + '...' if len(f) > 10 else f \n",
    "#                               for f in final_corr.columns], rotation=45, ha='right')\n",
    "#     axes[0,1].set_yticklabels([f[:10] + '...' if len(f) > 10 else f \n",
    "#                               for f in final_corr.columns])\n",
    "#     plt.colorbar(im, ax=axes[0,1], shrink=0.6)\n",
    "    \n",
    "#     # 3. Target correlations comparison\n",
    "#     original_target_corr = original_data.drop(columns=[target_col]).corrwith(original_data[target_col]).abs()\n",
    "#     final_target_corr = final_data.drop(columns=[target_col]).corrwith(final_data[target_col]).abs()\n",
    "    \n",
    "#     axes[1,0].hist(original_target_corr, bins=20, alpha=0.7, label='Original', color='lightcoral')\n",
    "#     axes[1,0].hist(final_target_corr, bins=10, alpha=0.7, label='Selected', color='lightgreen')\n",
    "#     axes[1,0].set_xlabel('Absolute Correlation with Target')\n",
    "#     axes[1,0].set_ylabel('Number of Features')\n",
    "#     axes[1,0].set_title('Distribution of Target Correlations')\n",
    "#     axes[1,0].legend()\n",
    "    \n",
    "#     # 4. Top features by target correlation\n",
    "#     top_features = final_target_corr.sort_values(ascending=False).head(10)\n",
    "#     y_pos = np.arange(len(top_features))\n",
    "    \n",
    "#     axes[1,1].barh(y_pos, top_features.values, color='lightblue')\n",
    "#     axes[1,1].set_yticks(y_pos)\n",
    "#     axes[1,1].set_yticklabels([f[:15] + '...' if len(f) > 15 else f \n",
    "#                               for f in top_features.index], fontsize=9)\n",
    "#     axes[1,1].set_xlabel('Absolute Correlation with Target')\n",
    "#     axes[1,1].set_title('Top 10 Selected Features by Target Correlation')\n",
    "#     axes[1,1].invert_yaxis()\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Summary statistics\n",
    "#     print(f\"\\n=== FEATURE SELECTION SUMMARY ===\")\n",
    "#     print(f\"Original features: {original_features}\")\n",
    "#     print(f\"Selected features: {final_features}\")\n",
    "#     print(f\"Reduction: {original_features - final_features} features ({((original_features - final_features) / original_features * 100):.1f}%)\")\n",
    "#     print(f\"Mean target correlation (original): {original_target_corr.mean():.4f}\")\n",
    "#     print(f\"Mean target correlation (selected): {final_target_corr.mean():.4f}\")\n",
    "#     print(f\"Max inter-feature correlation (selected): {final_corr.abs().max().max():.4f}\")\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_feature_selection_results(data, data_final_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "031ed35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_feature_selection_with_models(original_data, selected_data, target_col='Daily_Usage_Hours'):\n",
    "#     \"\"\"\n",
    "#     Compare model performance before and after feature selection\n",
    "#     \"\"\"\n",
    "#     print(\"=== VALIDATING FEATURE SELECTION WITH MODEL PERFORMANCE ===\")\n",
    "    \n",
    "#     from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#     from sklearn.ensemble import RandomForestRegressor\n",
    "#     from sklearn.linear_model import Ridge\n",
    "#     from sklearn.metrics import r2_score, mean_squared_error\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     for name, data in [('Original', original_data), ('Selected', selected_data)]:\n",
    "#         X = data.drop(columns=[target_col])\n",
    "#         y = data[target_col]\n",
    "        \n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "#         # Random Forest\n",
    "#         rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "#         rf_cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='r2')\n",
    "#         rf.fit(X_train, y_train)\n",
    "#         rf_pred = rf.predict(X_test)\n",
    "#         rf_r2 = r2_score(y_test, rf_pred)\n",
    "        \n",
    "#         # Ridge Regression\n",
    "#         ridge = Ridge(alpha=1.0)\n",
    "#         ridge_cv_scores = cross_val_score(ridge, X_train, y_train, cv=5, scoring='r2')\n",
    "#         ridge.fit(X_train, y_train)\n",
    "#         ridge_pred = ridge.predict(X_test)\n",
    "#         ridge_r2 = r2_score(y_test, ridge_pred)\n",
    "        \n",
    "#         results.append({\n",
    "#             'Dataset': name,\n",
    "#             'Features': X.shape[1],\n",
    "#             'RF_CV_R2': rf_cv_scores.mean(),\n",
    "#             'RF_Test_R2': rf_r2,\n",
    "#             'Ridge_CV_R2': ridge_cv_scores.mean(),\n",
    "#             'Ridge_Test_R2': ridge_r2\n",
    "#         })\n",
    "        \n",
    "#         print(f\"\\n{name} Dataset ({X.shape[1]} features):\")\n",
    "#         print(f\"  Random Forest - CV R²: {rf_cv_scores.mean():.4f}, Test R²: {rf_r2:.4f}\")\n",
    "#         print(f\"  Ridge Regression - CV R²: {ridge_cv_scores.mean():.4f}, Test R²: {ridge_r2:.4f}\")\n",
    "    \n",
    "#     results_df = pd.DataFrame(results)\n",
    "#     print(f\"\\n{results_df}\")\n",
    "    \n",
    "#     return results_df\n",
    "\n",
    "# # Validate feature selection\n",
    "# validation_results = validate_feature_selection_with_models(data, data_final_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96b8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
