{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8cfab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d98737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "data = pd.read_csv('Students_Cleaned_Encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a651e9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Study</th>\n",
       "      <th>Daily_Usage_Hours</th>\n",
       "      <th>Trust_in_AI_Tools</th>\n",
       "      <th>Impact_on_Grades</th>\n",
       "      <th>Awareness_Level</th>\n",
       "      <th>uses_coding_help</th>\n",
       "      <th>uses_assignments</th>\n",
       "      <th>uses_project_work</th>\n",
       "      <th>uses_mcq_practice</th>\n",
       "      <th>uses_exam_preparation</th>\n",
       "      <th>...</th>\n",
       "      <th>preferred_ai_tool_Gemini</th>\n",
       "      <th>preferred_ai_tool_Other</th>\n",
       "      <th>device_used_Laptop</th>\n",
       "      <th>device_used_Mobile</th>\n",
       "      <th>device_used_Tablet</th>\n",
       "      <th>internet_access_High</th>\n",
       "      <th>internet_access_Medium</th>\n",
       "      <th>internet_access_Poor</th>\n",
       "      <th>Do_Professors_Allow_Use_encoded</th>\n",
       "      <th>Willing_to_Pay_for_Access_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Study  Daily_Usage_Hours  Trust_in_AI_Tools  Impact_on_Grades  \\\n",
       "0              4                0.9                  2                 2   \n",
       "1              2                3.4                  3                -3   \n",
       "2              2                3.6                  5                 0   \n",
       "3              2                2.9                  5                 2   \n",
       "4              1                0.9                  1                 3   \n",
       "\n",
       "   Awareness_Level  uses_coding_help  uses_assignments  uses_project_work  \\\n",
       "0                9                 1                 1                  0   \n",
       "1                6                 0                 0                  0   \n",
       "2                1                 0                 0                  1   \n",
       "3                5                 0                 0                  0   \n",
       "4                8                 0                 0                  0   \n",
       "\n",
       "   uses_mcq_practice  uses_exam_preparation  ...  preferred_ai_tool_Gemini  \\\n",
       "0                  0                      0  ...                         0   \n",
       "1                  0                      0  ...                         0   \n",
       "2                  1                      0  ...                         1   \n",
       "3                  0                      0  ...                         1   \n",
       "4                  0                      0  ...                         0   \n",
       "\n",
       "   preferred_ai_tool_Other  device_used_Laptop  device_used_Mobile  \\\n",
       "0                        0                   0                   1   \n",
       "1                        1                   1                   0   \n",
       "2                        0                   0                   0   \n",
       "3                        0                   1                   0   \n",
       "4                        1                   1                   0   \n",
       "\n",
       "   device_used_Tablet  internet_access_High  internet_access_Medium  \\\n",
       "0                   0                     0                       0   \n",
       "1                   0                     0                       0   \n",
       "2                   1                     0                       0   \n",
       "3                   0                     1                       0   \n",
       "4                   0                     0                       1   \n",
       "\n",
       "   internet_access_Poor  Do_Professors_Allow_Use_encoded  \\\n",
       "0                     1                                0   \n",
       "1                     1                                1   \n",
       "2                     1                                0   \n",
       "3                     0                                1   \n",
       "4                     0                                1   \n",
       "\n",
       "   Willing_to_Pay_for_Access_encoded  \n",
       "0                                  1  \n",
       "1                                  0  \n",
       "2                                  0  \n",
       "3                                  0  \n",
       "4                                  1  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 5\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49dca86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year_of_Study', 'Daily_Usage_Hours', 'Trust_in_AI_Tools',\n",
       "       'Impact_on_Grades', 'Awareness_Level', 'uses_coding_help',\n",
       "       'uses_assignments', 'uses_project_work', 'uses_mcq_practice',\n",
       "       'uses_exam_preparation', 'uses_doubt_solving', 'uses_resume_writing',\n",
       "       'uses_content_writing', 'uses_learning_new_topics', 'uses_notes',\n",
       "       'ai_tool_bard', 'ai_tool_chatgpt', 'ai_tool_claude', 'ai_tool_copilot',\n",
       "       'ai_tool_gemini', 'ai_tool_midjourney', 'ai_tool_other',\n",
       "       'preferred_ai_tool_Bard', 'preferred_ai_tool_ChatGPT',\n",
       "       'preferred_ai_tool_Claude', 'preferred_ai_tool_Copilot',\n",
       "       'preferred_ai_tool_Gemini', 'preferred_ai_tool_Other',\n",
       "       'device_used_Laptop', 'device_used_Mobile', 'device_used_Tablet',\n",
       "       'internet_access_High', 'internet_access_Medium',\n",
       "       'internet_access_Poor', 'Do_Professors_Allow_Use_encoded',\n",
       "       'Willing_to_Pay_for_Access_encoded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b8393",
   "metadata": {},
   "source": [
    "# Daily_Usage_Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a935d877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SETUP ===\n",
      "Target variable: Daily_Usage_Hours\n",
      "Number of predictors: 35\n",
      "Target range: 0.5 to 5.0\n",
      "Target mean: 2.56\n",
      "Dataset shape: (3614, 36)\n"
     ]
    }
   ],
   "source": [
    "# Setup target variable and predictors\n",
    "target = 'Daily_Usage_Hours'\n",
    "X = data.drop(columns=[target])\n",
    "y = data[target]\n",
    "\n",
    "print(\"=== DATA SETUP ===\")\n",
    "print(f\"Target variable: {target}\")\n",
    "print(f\"Number of predictors: {len(X.columns)}\")\n",
    "print(f\"Target range: {y.min():.1f} to {y.max():.1f}\")\n",
    "print(f\"Target mean: {y.mean():.2f}\")\n",
    "print(f\"Dataset shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d85426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jqche\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import validation_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbaaa42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SPLIT ===\n",
      "Training set: 2891 samples\n",
      "Test set: 723 samples\n",
      "Training target range: 0.5 to 5.0\n",
      "Test target range: 0.5 to 5.0\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=None  # Since it's regression\n",
    ")\n",
    "\n",
    "print(\"=== DATA SPLIT ===\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training target range: {y_train.min():.1f} to {y_train.max():.1f}\")\n",
    "print(f\"Test target range: {y_test.min():.1f} to {y_test.max():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870183b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SCALING ANALYSIS ===\n",
      "\n",
      "📊 Binary/One-hot columns (31): DON'T need scaling\n",
      "   - uses_coding_help: [0, 1]\n",
      "   - uses_assignments: [0, 1]\n",
      "   - uses_project_work: [0, 1]\n",
      "   - uses_mcq_practice: [0, 1]\n",
      "   - uses_exam_preparation: [0, 1]\n",
      "   - uses_doubt_solving: [0, 1]\n",
      "   - uses_resume_writing: [0, 1]\n",
      "   - uses_content_writing: [0, 1]\n",
      "   - uses_learning_new_topics: [0, 1]\n",
      "   - uses_notes: [0, 1]\n",
      "   ... and 21 more binary columns\n",
      "\n",
      "📈 Continuous columns (4): NEED scaling\n",
      "   - Year_of_Study: range 1.00 to 4.00\n",
      "   - Trust_in_AI_Tools: range 1.00 to 5.00\n",
      "   - Impact_on_Grades: range -5.00 to 5.00\n",
      "   - Awareness_Level: range 1.00 to 10.00\n"
     ]
    }
   ],
   "source": [
    "# Check your data types and ranges\n",
    "def analyze_feature_scaling_needs(X):\n",
    "    \"\"\"Analyze which features need scaling\"\"\"\n",
    "    \n",
    "    print(\"=== FEATURE SCALING ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Identify binary/one-hot columns (only 0s and 1s)\n",
    "    binary_columns = []\n",
    "    continuous_columns = []\n",
    "    \n",
    "    for col in X.columns:\n",
    "        unique_vals = sorted(X[col].unique())\n",
    "        \n",
    "        # Check if column is binary (only 0 and 1)\n",
    "        if len(unique_vals) == 2 and set(unique_vals) == {0, 1}:\n",
    "            binary_columns.append(col)\n",
    "        # Check if column has more than 2 unique values\n",
    "        elif len(unique_vals) > 2:\n",
    "            continuous_columns.append(col)\n",
    "    \n",
    "    print(f\"📊 Binary/One-hot columns ({len(binary_columns)}): DON'T need scaling\")\n",
    "    for col in binary_columns[:10]:  # Show first 10\n",
    "        print(f\"   - {col}: {sorted(X[col].unique())}\")\n",
    "    if len(binary_columns) > 10:\n",
    "        print(f\"   ... and {len(binary_columns)-10} more binary columns\")\n",
    "    \n",
    "    print(f\"\\n📈 Continuous columns ({len(continuous_columns)}): NEED scaling\")\n",
    "    for col in continuous_columns:\n",
    "        print(f\"   - {col}: range {X[col].min():.2f} to {X[col].max():.2f}\")\n",
    "    \n",
    "    return binary_columns, continuous_columns\n",
    "\n",
    "# Analyze your features\n",
    "binary_cols, continuous_cols = analyze_feature_scaling_needs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66bf1059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Scaling 4 continuous features...\n",
      "✅ Selective scaling completed\n",
      "   - Binary columns: unchanged (0/1 values)\n",
      "   - Continuous columns: standardized (mean=0, std=1)\n"
     ]
    }
   ],
   "source": [
    "# Selective feature scaling - only scale continuous features\n",
    "def selective_feature_scaling(X_train, X_test, continuous_columns):\n",
    "    \"\"\"Scale only continuous features, leave binary features unchanged\"\"\"\n",
    "    \n",
    "    if not continuous_columns:\n",
    "        print(\"✅ No continuous features found - no scaling needed\")\n",
    "        return X_train.copy(), X_test.copy(), None\n",
    "    \n",
    "    print(f\"🔧 Scaling {len(continuous_columns)} continuous features...\")\n",
    "    \n",
    "    # Create copies\n",
    "    X_train_selective = X_train.copy()\n",
    "    X_test_selective = X_test.copy()\n",
    "    \n",
    "    # Scale only continuous columns\n",
    "    scaler = StandardScaler()\n",
    "    X_train_selective[continuous_columns] = scaler.fit_transform(X_train[continuous_columns])\n",
    "    X_test_selective[continuous_columns] = scaler.transform(X_test[continuous_columns])\n",
    "    \n",
    "    print(\"✅ Selective scaling completed\")\n",
    "    print(f\"   - Binary columns: unchanged (0/1 values)\")\n",
    "    print(f\"   - Continuous columns: standardized (mean=0, std=1)\")\n",
    "    \n",
    "    return X_train_selective, X_test_selective, scaler\n",
    "\n",
    "# Apply selective scaling\n",
    "X_train_selective, X_test_selective, selective_scaler = selective_feature_scaling(X_train, X_test, continuous_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66e8c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73c87402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Study</th>\n",
       "      <th>Trust_in_AI_Tools</th>\n",
       "      <th>Impact_on_Grades</th>\n",
       "      <th>Awareness_Level</th>\n",
       "      <th>uses_coding_help</th>\n",
       "      <th>uses_assignments</th>\n",
       "      <th>uses_project_work</th>\n",
       "      <th>uses_mcq_practice</th>\n",
       "      <th>uses_exam_preparation</th>\n",
       "      <th>uses_doubt_solving</th>\n",
       "      <th>uses_resume_writing</th>\n",
       "      <th>uses_content_writing</th>\n",
       "      <th>uses_learning_new_topics</th>\n",
       "      <th>uses_notes</th>\n",
       "      <th>ai_tool_bard</th>\n",
       "      <th>ai_tool_chatgpt</th>\n",
       "      <th>ai_tool_claude</th>\n",
       "      <th>ai_tool_copilot</th>\n",
       "      <th>ai_tool_gemini</th>\n",
       "      <th>ai_tool_midjourney</th>\n",
       "      <th>ai_tool_other</th>\n",
       "      <th>preferred_ai_tool_Bard</th>\n",
       "      <th>preferred_ai_tool_ChatGPT</th>\n",
       "      <th>preferred_ai_tool_Claude</th>\n",
       "      <th>preferred_ai_tool_Copilot</th>\n",
       "      <th>preferred_ai_tool_Gemini</th>\n",
       "      <th>preferred_ai_tool_Other</th>\n",
       "      <th>device_used_Laptop</th>\n",
       "      <th>device_used_Mobile</th>\n",
       "      <th>device_used_Tablet</th>\n",
       "      <th>internet_access_High</th>\n",
       "      <th>internet_access_Medium</th>\n",
       "      <th>internet_access_Poor</th>\n",
       "      <th>Do_Professors_Allow_Use_encoded</th>\n",
       "      <th>Willing_to_Pay_for_Access_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>1.365877</td>\n",
       "      <td>-1.422853</td>\n",
       "      <td>-0.834875</td>\n",
       "      <td>0.061062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>0.462480</td>\n",
       "      <td>-0.021808</td>\n",
       "      <td>1.694245</td>\n",
       "      <td>-0.624490</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>-0.440918</td>\n",
       "      <td>0.678715</td>\n",
       "      <td>-0.413355</td>\n",
       "      <td>-1.652817</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>-1.344316</td>\n",
       "      <td>-0.722331</td>\n",
       "      <td>-2.099434</td>\n",
       "      <td>0.061062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>-0.440918</td>\n",
       "      <td>0.678715</td>\n",
       "      <td>-0.834875</td>\n",
       "      <td>-1.310041</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year_of_Study  Trust_in_AI_Tools  Impact_on_Grades  Awareness_Level  \\\n",
       "1807       1.365877          -1.422853         -0.834875         0.061062   \n",
       "1665       0.462480          -0.021808          1.694245        -0.624490   \n",
       "1721      -0.440918           0.678715         -0.413355        -1.652817   \n",
       "1632      -1.344316          -0.722331         -2.099434         0.061062   \n",
       "1233      -0.440918           0.678715         -0.834875        -1.310041   \n",
       "\n",
       "      uses_coding_help  uses_assignments  uses_project_work  \\\n",
       "1807                 0                 0                  1   \n",
       "1665                 1                 1                  0   \n",
       "1721                 1                 0                  1   \n",
       "1632                 0                 0                  1   \n",
       "1233                 1                 0                  0   \n",
       "\n",
       "      uses_mcq_practice  uses_exam_preparation  uses_doubt_solving  \\\n",
       "1807                  0                      0                   1   \n",
       "1665                  0                      0                   0   \n",
       "1721                  0                      1                   0   \n",
       "1632                  0                      0                   0   \n",
       "1233                  0                      0                   0   \n",
       "\n",
       "      uses_resume_writing  uses_content_writing  uses_learning_new_topics  \\\n",
       "1807                    0                     0                         0   \n",
       "1665                    1                     0                         0   \n",
       "1721                    0                     0                         0   \n",
       "1632                    0                     0                         1   \n",
       "1233                    0                     1                         0   \n",
       "\n",
       "      uses_notes  ai_tool_bard  ai_tool_chatgpt  ai_tool_claude  \\\n",
       "1807           0             0                0               0   \n",
       "1665           0             0                0               0   \n",
       "1721           0             0                0               0   \n",
       "1632           0             0                0               0   \n",
       "1233           0             0                0               0   \n",
       "\n",
       "      ai_tool_copilot  ai_tool_gemini  ai_tool_midjourney  ai_tool_other  \\\n",
       "1807                0               1                   0              0   \n",
       "1665                1               0                   0              0   \n",
       "1721                0               0                   0              1   \n",
       "1632                1               0                   0              0   \n",
       "1233                0               1                   0              0   \n",
       "\n",
       "      preferred_ai_tool_Bard  preferred_ai_tool_ChatGPT  \\\n",
       "1807                       0                          1   \n",
       "1665                       0                          0   \n",
       "1721                       0                          0   \n",
       "1632                       0                          0   \n",
       "1233                       0                          0   \n",
       "\n",
       "      preferred_ai_tool_Claude  preferred_ai_tool_Copilot  \\\n",
       "1807                         0                          0   \n",
       "1665                         1                          0   \n",
       "1721                         0                          1   \n",
       "1632                         1                          0   \n",
       "1233                         0                          0   \n",
       "\n",
       "      preferred_ai_tool_Gemini  preferred_ai_tool_Other  device_used_Laptop  \\\n",
       "1807                         0                        0                   1   \n",
       "1665                         0                        0                   0   \n",
       "1721                         0                        0                   0   \n",
       "1632                         0                        0                   1   \n",
       "1233                         1                        0                   0   \n",
       "\n",
       "      device_used_Mobile  device_used_Tablet  internet_access_High  \\\n",
       "1807                   0                   0                     1   \n",
       "1665                   1                   0                     0   \n",
       "1721                   1                   0                     0   \n",
       "1632                   0                   0                     0   \n",
       "1233                   1                   0                     0   \n",
       "\n",
       "      internet_access_Medium  internet_access_Poor  \\\n",
       "1807                       0                     0   \n",
       "1665                       1                     0   \n",
       "1721                       0                     1   \n",
       "1632                       1                     0   \n",
       "1233                       1                     0   \n",
       "\n",
       "      Do_Professors_Allow_Use_encoded  Willing_to_Pay_for_Access_encoded  \n",
       "1807                                0                                  0  \n",
       "1665                                0                                  1  \n",
       "1721                                0                                  1  \n",
       "1632                                1                                  1  \n",
       "1233                                1                                  0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 5 of train scaled\n",
    "pd.DataFrame(X_train_selective, columns=X.columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90bb003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING MODELS WITH SMART SCALING ===\n",
      "\n",
      "Training Linear Regression...\n",
      "   Scaling: Yes - Linear models benefit from scaling continuous features\n",
      "   Test R²: 0.0143 | RMSE: 1.2094\n",
      "\n",
      "Training Ridge Regression...\n",
      "   Scaling: Yes - Regularization requires scaled features\n",
      "   Test R²: 0.0145 | RMSE: 1.2093\n",
      "\n",
      "Training Lasso Regression...\n",
      "   Scaling: Yes - Regularization requires scaled features\n",
      "   Test R²: -0.0009 | RMSE: 1.2187\n",
      "\n",
      "Training Random Forest...\n",
      "   Scaling: No - Tree-based models are scale-invariant\n",
      "   Test R²: 0.4464 | RMSE: 0.9063\n",
      "\n",
      "Training Gradient Boosting...\n",
      "   Scaling: No - Tree-based models are scale-invariant\n",
      "   Test R²: 0.1040 | RMSE: 1.1531\n",
      "\n",
      "Training Decision Tree...\n",
      "   Scaling: No - Tree-based models are scale-invariant\n",
      "   Test R²: 0.0330 | RMSE: 1.1979\n",
      "\n",
      "Training K-Nearest Neighbors...\n",
      "   Scaling: Yes - Distance-based algorithm sensitive to scale\n",
      "   Test R²: 0.1563 | RMSE: 1.1189\n",
      "\n",
      "Training Support Vector Regression...\n",
      "   Scaling: Yes - SVM is sensitive to feature scale\n",
      "   Test R²: 0.1807 | RMSE: 1.1026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Updated training function with proper scaling logic\n",
    "def train_models_with_smart_scaling(X_train, X_test, y_train, y_test, \n",
    "                                   X_train_selective, X_test_selective, binary_cols, continuous_cols):\n",
    "    \"\"\"Train models with appropriate scaling strategy\"\"\"\n",
    "    \n",
    "    # Define models and their scaling requirements\n",
    "    models_config = {\n",
    "        'Linear Regression': {\n",
    "            'model': LinearRegression(),\n",
    "            'needs_scaling': len(continuous_cols) > 0,\n",
    "            'reason': 'Linear models benefit from scaling continuous features'\n",
    "        },\n",
    "        'Ridge Regression': {\n",
    "            'model': Ridge(alpha=1.0),\n",
    "            'needs_scaling': len(continuous_cols) > 0,\n",
    "            'reason': 'Regularization requires scaled features'\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'model': Lasso(alpha=1.0),\n",
    "            'needs_scaling': len(continuous_cols) > 0,\n",
    "            'reason': 'Regularization requires scaled features'\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'needs_scaling': False,\n",
    "            'reason': 'Tree-based models are scale-invariant'\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'needs_scaling': False,\n",
    "            'reason': 'Tree-based models are scale-invariant'\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'model': DecisionTreeRegressor(random_state=42),\n",
    "            'needs_scaling': False,\n",
    "            'reason': 'Tree-based models are scale-invariant'\n",
    "        },\n",
    "        'K-Nearest Neighbors': {\n",
    "            'model': KNeighborsRegressor(n_neighbors=5),\n",
    "            'needs_scaling': True,\n",
    "            'reason': 'Distance-based algorithm sensitive to scale'\n",
    "        },\n",
    "        'Support Vector Regression': {\n",
    "            'model': SVR(kernel='rbf'),\n",
    "            'needs_scaling': True,\n",
    "            'reason': 'SVM is sensitive to feature scale'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    print(\"=== TRAINING MODELS WITH SMART SCALING ===\\n\")\n",
    "    \n",
    "    for name, config in models_config.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        print(f\"   Scaling: {'Yes' if config['needs_scaling'] else 'No'} - {config['reason']}\")\n",
    "        \n",
    "        # Choose appropriate data\n",
    "        if config['needs_scaling']:\n",
    "            X_tr, X_te = X_train_selective, X_test_selective\n",
    "        else:\n",
    "            X_tr, X_te = X_train, X_test\n",
    "        \n",
    "        # Train model\n",
    "        model = config['model']\n",
    "        model.fit(X_tr, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train = model.predict(X_tr)\n",
    "        y_pred_test = model.predict(X_te)\n",
    "        \n",
    "        # Metrics\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_tr, y_train, cv=5, scoring='r2')\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'test_r2': test_r2,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_mae': test_mae,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'used_scaling': config['needs_scaling']\n",
    "        }\n",
    "        \n",
    "        print(f\"   Test R²: {test_r2:.4f} | RMSE: {test_rmse:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train models with smart scaling\n",
    "smart_results = train_models_with_smart_scaling(X_train, X_test, y_train, y_test,\n",
    "                                               X_train_selective, X_test_selective, \n",
    "                                               binary_cols, continuous_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b5a08f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCALING RECOMMENDATIONS FOR YOUR DATA ===\n",
      "\n",
      "🔍 Your Data Characteristics:\n",
      "   - Binary/One-hot encoded columns: 31 (don't scale)\n",
      "   - Continuous columns: 4 (scale if needed)\n",
      "\n",
      "✅ Recommended Approach:\n",
      "   1. DON'T scale binary/one-hot columns (already 0/1)\n",
      "   2. ONLY scale continuous columns for distance-based algorithms\n",
      "   3. Tree-based models: No scaling needed\n",
      "   4. Linear/SVM/KNN models: Scale continuous features only\n",
      "\n",
      "📊 Models that performed best:\n",
      "   1. Random Forest: R²=0.4464 (without scaling)\n",
      "   2. Support Vector Regression: R²=0.1807 (with scaling)\n",
      "   3. K-Nearest Neighbors: R²=0.1563 (with scaling)\n"
     ]
    }
   ],
   "source": [
    "# Summary of scaling recommendations\n",
    "print(\"=== SCALING RECOMMENDATIONS FOR YOUR DATA ===\\n\")\n",
    "\n",
    "print(\"🔍 Your Data Characteristics:\")\n",
    "print(f\"   - Binary/One-hot encoded columns: {len(binary_cols)} (don't scale)\")\n",
    "print(f\"   - Continuous columns: {len(continuous_cols)} (scale if needed)\")\n",
    "\n",
    "print(\"\\n✅ Recommended Approach:\")\n",
    "print(\"   1. DON'T scale binary/one-hot columns (already 0/1)\")\n",
    "print(\"   2. ONLY scale continuous columns for distance-based algorithms\")\n",
    "print(\"   3. Tree-based models: No scaling needed\")\n",
    "print(\"   4. Linear/SVM/KNN models: Scale continuous features only\")\n",
    "\n",
    "print(f\"\\n📊 Models that performed best:\")\n",
    "sorted_results = sorted(smart_results.items(), key=lambda x: x[1]['test_r2'], reverse=True)\n",
    "for i, (name, metrics) in enumerate(sorted_results[:3], 1):\n",
    "    scaling_status = \"with scaling\" if metrics['used_scaling'] else \"without scaling\"\n",
    "    print(f\"   {i}. {name}: R²={metrics['test_r2']:.4f} ({scaling_status})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c441d2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL PERFORMANCE ANALYSIS ===\n",
      "\n",
      "🏆 Performance Ranking:\n",
      " 1. Random Forest             | R²: 0.4464 | RMSE: 0.9063 | Moderate\n",
      " 2. Support Vector Regression | R²: 0.1807 | RMSE: 1.1026 | Poor\n",
      " 3. K-Nearest Neighbors       | R²: 0.1563 | RMSE: 1.1189 | Poor\n",
      " 4. Gradient Boosting         | R²: 0.1040 | RMSE: 1.1531 | Poor\n",
      " 5. Decision Tree             | R²: 0.0330 | RMSE: 1.1979 | Poor\n",
      " 6. Ridge Regression          | R²: 0.0145 | RMSE: 1.2093 | Poor\n",
      " 7. Linear Regression         | R²: 0.0143 | RMSE: 1.2094 | Poor\n",
      " 8. Lasso Regression          | R²: -0.0009 | RMSE: 1.2187 | Poor\n",
      "\n",
      "🔍 Key Observations:\n",
      "   • Best model: Random Forest (R² = 0.4464)\n",
      "   • Tree-based models dominate top positions\n",
      "   • Linear models struggle (R² < 0.02) → Non-linear relationships exist\n",
      "   • Room for improvement: Current best explains 44.6% of variance\n"
     ]
    }
   ],
   "source": [
    "# Detailed analysis of model performance\n",
    "def analyze_model_performance(smart_results):\n",
    "    \"\"\"Analyze and interpret model performance\"\"\"\n",
    "    \n",
    "    print(\"=== MODEL PERFORMANCE ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Sort results by test R²\n",
    "    sorted_results = sorted(smart_results.items(), key=lambda x: x[1]['test_r2'], reverse=True)\n",
    "    \n",
    "    print(\"🏆 Performance Ranking:\")\n",
    "    for i, (name, metrics) in enumerate(sorted_results, 1):\n",
    "        performance_level = \"Excellent\" if metrics['test_r2'] > 0.7 else \\\n",
    "                           \"Good\" if metrics['test_r2'] > 0.5 else \\\n",
    "                           \"Moderate\" if metrics['test_r2'] > 0.3 else \\\n",
    "                           \"Poor\"\n",
    "        \n",
    "        print(f\"{i:2d}. {name:<25} | R²: {metrics['test_r2']:6.4f} | RMSE: {metrics['test_rmse']:6.4f} | {performance_level}\")\n",
    "    \n",
    "    print(f\"\\n🔍 Key Observations:\")\n",
    "    best_model = sorted_results[0]\n",
    "    print(f\"   • Best model: {best_model[0]} (R² = {best_model[1]['test_r2']:.4f})\")\n",
    "    print(f\"   • Tree-based models dominate top positions\")\n",
    "    print(f\"   • Linear models struggle (R² < 0.02) → Non-linear relationships exist\")\n",
    "    print(f\"   • Room for improvement: Current best explains {best_model[1]['test_r2']*100:.1f}% of variance\")\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "# Analyze current results\n",
    "performance_analysis = analyze_model_performance(smart_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f76d1543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE IMPORTANCE ANALYSIS ===\n",
      "\n",
      "🔍 Top 15 Most Important Features:\n",
      " 4. Awareness_Level                     | 0.1376\n",
      " 3. Impact_on_Grades                    | 0.1135\n",
      " 2. Trust_in_AI_Tools                   | 0.0895\n",
      " 1. Year_of_Study                       | 0.0774\n",
      "34. Do_Professors_Allow_Use_encoded     | 0.0343\n",
      "19. ai_tool_gemini                      | 0.0267\n",
      "16. ai_tool_chatgpt                     | 0.0263\n",
      "28. device_used_Laptop                  | 0.0246\n",
      "25. preferred_ai_tool_Copilot           | 0.0246\n",
      "23. preferred_ai_tool_ChatGPT           | 0.0240\n",
      "12. uses_content_writing                | 0.0230\n",
      "29. device_used_Mobile                  | 0.0227\n",
      "27. preferred_ai_tool_Other             | 0.0226\n",
      "30. device_used_Tablet                  | 0.0224\n",
      "26. preferred_ai_tool_Gemini            | 0.0222\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature importance analysis for Random Forest (best model)\n",
    "def analyze_feature_importance(smart_results, X):\n",
    "    \"\"\"Analyze feature importance for the best performing model\"\"\"\n",
    "    \n",
    "    print(\"=== FEATURE IMPORTANCE ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Get Random Forest model (best performer)\n",
    "    rf_model = smart_results['Random Forest']['model']\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = rf_model.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"🔍 Top 15 Most Important Features:\")\n",
    "    for i, row in importance_df.head(15).iterrows():\n",
    "        print(f\"{row.name+1:2d}. {row['feature']:<35} | {row['importance']:.4f}\")\n",
    "    \n",
    "    # Visualize top features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = importance_df.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'][::-1], color='skyblue')\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'][::-1])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Random Forest - Top 15 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance = analyze_feature_importance(smart_results, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0699b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HYPERPARAMETER OPTIMIZATION ===\n",
      "\n",
      "🔧 Optimizing Random Forest...\n",
      "   ✅ Best CV Score: 0.3749\n",
      "   ✅ Test R²: 0.4581 | RMSE: 0.8967\n",
      "   ✅ Best Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "🔧 Optimizing Gradient Boosting...\n",
      "   ✅ Best CV Score: 0.3555\n",
      "   ✅ Test R²: 0.3989 | RMSE: 0.9444\n",
      "   ✅ Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 0.9}\n",
      "\n",
      "🔧 Optimizing Support Vector Regression...\n",
      "   ✅ Best CV Score: 0.1563\n",
      "   ✅ Test R²: 0.2337 | RMSE: 1.0664\n",
      "   ✅ Best Parameters: {'C': 10, 'epsilon': 0.2, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization for top models\n",
    "def optimize_top_models(X_train, X_test, y_train, y_test, X_train_selective, X_test_selective):\n",
    "    \"\"\"Optimize hyperparameters for top performing models\"\"\"\n",
    "    \n",
    "    print(\"=== HYPERPARAMETER OPTIMIZATION ===\\n\")\n",
    "    \n",
    "    # Define parameter grids for top models\n",
    "    optimization_configs = {\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 15, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2', None]\n",
    "            },\n",
    "            'data': (X_train, X_test),\n",
    "            'cv_folds': 5\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.05, 0.1, 0.15],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'data': (X_train, X_test),\n",
    "            'cv_folds': 5\n",
    "        },\n",
    "        'Support Vector Regression': {\n",
    "            'model': SVR(),\n",
    "            'params': {\n",
    "                'kernel': ['rbf', 'poly'],\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "                'epsilon': [0.01, 0.1, 0.2]\n",
    "            },\n",
    "            'data': (X_train_selective, X_test_selective),\n",
    "            'cv_folds': 3  # Fewer folds for SVR (computationally expensive)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    optimized_results = {}\n",
    "    \n",
    "    for name, config in optimization_configs.items():\n",
    "        print(f\"🔧 Optimizing {name}...\")\n",
    "        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            config['model'],\n",
    "            config['params'],\n",
    "            cv=config['cv_folds'],\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        X_tr, X_te = config['data']\n",
    "        grid_search.fit(X_tr, y_train)\n",
    "        \n",
    "        # Evaluate best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred_test = best_model.predict(X_te)\n",
    "        \n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        \n",
    "        optimized_results[name] = {\n",
    "            'best_model': best_model,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'test_r2': test_r2,\n",
    "            'test_rmse': test_rmse\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✅ Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"   ✅ Test R²: {test_r2:.4f} | RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"   ✅ Best Parameters: {grid_search.best_params_}\")\n",
    "        print()\n",
    "    \n",
    "    return optimized_results\n",
    "\n",
    "# Optimize top models\n",
    "optimized_models = optimize_top_models(X_train, X_test, y_train, y_test, \n",
    "                                      X_train_selective, X_test_selective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b15af8c",
   "metadata": {},
   "source": [
    "Looking at your hyperparameter optimization results, here's what they tell us:\n",
    "## 📊 Key Insights from Hyperparameter Tuning\n",
    "\n",
    "**🎯 Performance Improvements:**\n",
    "- **Random Forest improved**: R² from 0.4464 → 0.4581 (+0.0117)\n",
    "- **Gradient Boosting improved**: R² from 0.1040 → 0.3989 (+0.2949) - **Significant improvement!**\n",
    "- **SVR improved**: R² from 0.1807 → 0.2337 (+0.0530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7f31990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HYPERPARAMETER OPTIMIZATION IMPACT ===\n",
      "\n",
      "🏆 Optimization Results (sorted by R² improvement):\n",
      "================================================================================\n",
      "Gradient Boosting        \n",
      "   Original R²: 0.1040 → Optimized R²: 0.3989\n",
      "   Improvement: +0.2949 | 🚀 Significant\n",
      "   Best params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 0.9}\n",
      "\n",
      "Support Vector Regression\n",
      "   Original R²: 0.1807 → Optimized R²: 0.2337\n",
      "   Improvement: +0.0529 | ✅ Good\n",
      "   Best params: {'C': 10, 'epsilon': 0.2, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n",
      "Random Forest            \n",
      "   Original R²: 0.4464 → Optimized R²: 0.4581\n",
      "   Improvement: +0.0117 | ✅ Good\n",
      "   Best params: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare before/after hyperparameter tuning\n",
    "def compare_optimization_results(smart_results, optimized_models):\n",
    "    \"\"\"Compare performance before and after hyperparameter optimization\"\"\"\n",
    "    \n",
    "    print(\"=== HYPERPARAMETER OPTIMIZATION IMPACT ===\\n\")\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name in optimized_models.keys():\n",
    "        if model_name in smart_results:\n",
    "            original_r2 = smart_results[model_name]['test_r2']\n",
    "            original_rmse = smart_results[model_name]['test_rmse']\n",
    "            \n",
    "            optimized_r2 = optimized_models[model_name]['test_r2']\n",
    "            optimized_rmse = optimized_models[model_name]['test_rmse']\n",
    "            \n",
    "            r2_improvement = optimized_r2 - original_r2\n",
    "            rmse_improvement = original_rmse - optimized_rmse  # Lower RMSE is better\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Original_R2': original_r2,\n",
    "                'Optimized_R2': optimized_r2,\n",
    "                'R2_Improvement': r2_improvement,\n",
    "                'RMSE_Improvement': rmse_improvement,\n",
    "                'Best_Params': optimized_models[model_name]['best_params']\n",
    "            })\n",
    "    \n",
    "    # Sort by R² improvement\n",
    "    comparison_data.sort(key=lambda x: x['R2_Improvement'], reverse=True)\n",
    "    \n",
    "    print(\"🏆 Optimization Results (sorted by R² improvement):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for result in comparison_data:\n",
    "        improvement_status = \"🚀 Significant\" if result['R2_Improvement'] > 0.1 else \\\n",
    "                           \"✅ Good\" if result['R2_Improvement'] > 0.01 else \\\n",
    "                           \"⚡ Minor\" if result['R2_Improvement'] > 0 else \\\n",
    "                           \"❌ None\"\n",
    "        \n",
    "        print(f\"{result['Model']:<25}\")\n",
    "        print(f\"   Original R²: {result['Original_R2']:.4f} → Optimized R²: {result['Optimized_R2']:.4f}\")\n",
    "        print(f\"   Improvement: {result['R2_Improvement']:+.4f} | {improvement_status}\")\n",
    "        print(f\"   Best params: {result['Best_Params']}\")\n",
    "        print()\n",
    "    \n",
    "    return comparison_data\n",
    "\n",
    "# Compare optimization results\n",
    "comparison_results = compare_optimization_results(smart_results, optimized_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5023d6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING ENSEMBLE MODEL ===\n",
      "\n",
      "📊 Ensemble Composition:\n",
      "   Random Forest: 0.420 weight\n",
      "   Gradient Boosting: 0.366 weight\n",
      "   Support Vector Regression: 0.214 weight\n",
      "\n",
      "🎯 Ensemble Performance:\n",
      "   R²: 0.4351\n",
      "   RMSE: 0.9156\n",
      "   MAE: 0.6455\n",
      "\n",
      "📈 vs Best Individual Model:\n",
      "   Improvement: -0.0230 R²\n",
      "   Status: ❌ Worse\n"
     ]
    }
   ],
   "source": [
    "# Create ensemble model combining best performers\n",
    "def create_ensemble_model(optimized_models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Create ensemble model from optimized models\"\"\"\n",
    "    \n",
    "    print(\"=== CREATING ENSEMBLE MODEL ===\\n\")\n",
    "    \n",
    "    # Get the top 3 models\n",
    "    top_models = ['Random Forest', 'Gradient Boosting', 'Support Vector Regression']\n",
    "    \n",
    "    # Create individual predictions\n",
    "    predictions = {}\n",
    "    weights = {}\n",
    "    \n",
    "    for model_name in top_models:\n",
    "        if model_name in optimized_models:\n",
    "            model = optimized_models[model_name]['best_model']\n",
    "            \n",
    "            # Use appropriate data (scaled for SVR, unscaled for tree models)\n",
    "            if model_name == 'Support Vector Regression':\n",
    "                pred = model.predict(X_test_selective)\n",
    "            else:\n",
    "                pred = model.predict(X_test)\n",
    "            \n",
    "            predictions[model_name] = pred\n",
    "            \n",
    "            # Weight by R² performance\n",
    "            weights[model_name] = optimized_models[model_name]['test_r2']\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = sum(weights.values())\n",
    "    for model_name in weights:\n",
    "        weights[model_name] = weights[model_name] / total_weight\n",
    "    \n",
    "    # Create weighted ensemble prediction\n",
    "    ensemble_pred = np.zeros(len(y_test))\n",
    "    \n",
    "    print(\"📊 Ensemble Composition:\")\n",
    "    for model_name, weight in weights.items():\n",
    "        ensemble_pred += weight * predictions[model_name]\n",
    "        print(f\"   {model_name}: {weight:.3f} weight\")\n",
    "    \n",
    "    # Calculate ensemble metrics\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_pred)\n",
    "    ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))\n",
    "    ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n",
    "    \n",
    "    print(f\"\\n🎯 Ensemble Performance:\")\n",
    "    print(f\"   R²: {ensemble_r2:.4f}\")\n",
    "    print(f\"   RMSE: {ensemble_rmse:.4f}\")\n",
    "    print(f\"   MAE: {ensemble_mae:.4f}\")\n",
    "    \n",
    "    # Compare with best individual model\n",
    "    best_individual_r2 = max([optimized_models[m]['test_r2'] for m in optimized_models.keys()])\n",
    "    improvement = ensemble_r2 - best_individual_r2\n",
    "    \n",
    "    print(f\"\\n📈 vs Best Individual Model:\")\n",
    "    print(f\"   Improvement: {improvement:+.4f} R²\")\n",
    "    print(f\"   Status: {'✅ Better' if improvement > 0 else '❌ Worse'}\")\n",
    "    \n",
    "    return ensemble_pred, ensemble_r2, ensemble_rmse, weights\n",
    "\n",
    "# Create ensemble model\n",
    "ensemble_pred, ensemble_r2, ensemble_rmse, ensemble_weights = create_ensemble_model(\n",
    "    optimized_models, X_train, X_test, y_train, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4b69d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADVANCED FEATURE ENGINEERING ===\n",
      "\n",
      "🔍 Using top 10 features for interactions\n",
      "✅ Created 8 advanced features:\n",
      "   • Awareness_Level_x_Impact_on_Grades\n",
      "   • Awareness_Level_x_Trust_in_AI_Tools\n",
      "   • Impact_on_Grades_x_Trust_in_AI_Tools\n",
      "   • usage_diversity\n",
      "   • usage_intensity\n",
      "   • tech_adoption_score\n",
      "   • academic_integration_ratio\n",
      "   • trust_awareness_synergy\n",
      "📊 Dataset expanded: 35 → 43 features\n"
     ]
    }
   ],
   "source": [
    "# Advanced feature engineering based on insights\n",
    "def advanced_feature_engineering(X, y, feature_importance):\n",
    "    \"\"\"Create advanced engineered features\"\"\"\n",
    "    \n",
    "    print(\"=== ADVANCED FEATURE ENGINEERING ===\\n\")\n",
    "    \n",
    "    X_advanced = X.copy()\n",
    "    \n",
    "    # 1. Get top features from Random Forest\n",
    "    top_features = feature_importance.head(10)['feature'].tolist()\n",
    "    print(f\"🔍 Using top {len(top_features)} features for interactions\")\n",
    "    \n",
    "    # 2. Create interaction features (top features only)\n",
    "    interaction_count = 0\n",
    "    for i, feat1 in enumerate(top_features[:5]):  # Limit to avoid explosion\n",
    "        for feat2 in top_features[i+1:3]:\n",
    "            if feat1 in X.columns and feat2 in X.columns:\n",
    "                interaction_name = f\"{feat1}_x_{feat2}\"\n",
    "                X_advanced[interaction_name] = X[feat1] * X[feat2]\n",
    "                interaction_count += 1\n",
    "    \n",
    "    # 3. Usage intensity features\n",
    "    usage_cols = [col for col in X.columns if col.startswith('uses_')]\n",
    "    if usage_cols:\n",
    "        X_advanced['usage_diversity'] = X[usage_cols].sum(axis=1)\n",
    "        X_advanced['usage_intensity'] = X[usage_cols].mean(axis=1)\n",
    "    \n",
    "    # 4. Technology adoption score\n",
    "    tech_cols = [col for col in X.columns if col.startswith('ai_tool_')]\n",
    "    if tech_cols:\n",
    "        X_advanced['tech_adoption_score'] = X[tech_cols].sum(axis=1)\n",
    "    \n",
    "    # 5. Academic integration ratio\n",
    "    academic_cols = ['uses_assignments', 'uses_project_work', 'uses_exam_preparation']\n",
    "    available_academic = [col for col in academic_cols if col in X.columns]\n",
    "    if available_academic and usage_cols:\n",
    "        academic_sum = X[available_academic].sum(axis=1)\n",
    "        total_usage = X[usage_cols].sum(axis=1)\n",
    "        # Avoid division by zero\n",
    "        X_advanced['academic_integration_ratio'] = np.where(\n",
    "            total_usage > 0, \n",
    "            academic_sum / total_usage, \n",
    "            0\n",
    "        )\n",
    "    \n",
    "    # 6. Trust-awareness synergy (if available)\n",
    "    if 'Trust_in_AI_Tools' in X.columns and 'Awareness_Level' in X.columns:\n",
    "        X_advanced['trust_awareness_synergy'] = (\n",
    "            X['Trust_in_AI_Tools'] * X['Awareness_Level'] / \n",
    "            (X['Trust_in_AI_Tools'] + X['Awareness_Level'] + 1)  # +1 to avoid division by zero\n",
    "        )\n",
    "    \n",
    "    new_features = [col for col in X_advanced.columns if col not in X.columns]\n",
    "    \n",
    "    print(f\"✅ Created {len(new_features)} advanced features:\")\n",
    "    for feat in new_features:\n",
    "        print(f\"   • {feat}\")\n",
    "    \n",
    "    print(f\"📊 Dataset expanded: {X.shape[1]} → {X_advanced.shape[1]} features\")\n",
    "    \n",
    "    return X_advanced\n",
    "\n",
    "# Create advanced features\n",
    "X_advanced = advanced_feature_engineering(X, y, feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59575a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING ADVANCED FEATURES ===\n",
      "\n",
      "📈 Advanced Features Performance:\n",
      "   Test R²: 0.4737\n",
      "   Test RMSE: 0.8838\n",
      "   Test MAE: 0.6337\n",
      "   CV R² (mean±std): 0.3852±0.0244\n",
      "\n",
      "🔄 Comparison with Optimized Random Forest:\n",
      "   Previous R²: 0.4581\n",
      "   Advanced R²: 0.4737\n",
      "   Improvement: +0.0155\n",
      "   Status: ✅ Improvement\n"
     ]
    }
   ],
   "source": [
    "# Test advanced features with optimized Random Forest\n",
    "def test_advanced_features(X_advanced, y):\n",
    "    \"\"\"Test advanced features with the best model\"\"\"\n",
    "    \n",
    "    print(\"=== TESTING ADVANCED FEATURES ===\\n\")\n",
    "    \n",
    "    # Split advanced dataset\n",
    "    X_train_adv, X_test_adv, y_train_adv, y_test_adv = train_test_split(\n",
    "        X_advanced, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Use best Random Forest parameters from optimization\n",
    "    best_rf_params = optimized_models['Random Forest']['best_params']\n",
    "    \n",
    "    # Train advanced Random Forest\n",
    "    rf_advanced = RandomForestRegressor(**best_rf_params, random_state=42)\n",
    "    rf_advanced.fit(X_train_adv, y_train_adv)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_adv = rf_advanced.predict(X_test_adv)\n",
    "    \n",
    "    # Metrics\n",
    "    r2_adv = r2_score(y_test_adv, y_pred_adv)\n",
    "    rmse_adv = np.sqrt(mean_squared_error(y_test_adv, y_pred_adv))\n",
    "    mae_adv = mean_absolute_error(y_test_adv, y_pred_adv)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores_adv = cross_val_score(rf_advanced, X_train_adv, y_train_adv, cv=5, scoring='r2')\n",
    "    \n",
    "    print(\"📈 Advanced Features Performance:\")\n",
    "    print(f\"   Test R²: {r2_adv:.4f}\")\n",
    "    print(f\"   Test RMSE: {rmse_adv:.4f}\")\n",
    "    print(f\"   Test MAE: {mae_adv:.4f}\")\n",
    "    print(f\"   CV R² (mean±std): {cv_scores_adv.mean():.4f}±{cv_scores_adv.std():.4f}\")\n",
    "    \n",
    "    # Compare with previous best\n",
    "    original_r2 = optimized_models['Random Forest']['test_r2']\n",
    "    improvement = r2_adv - original_r2\n",
    "    \n",
    "    print(f\"\\n🔄 Comparison with Optimized Random Forest:\")\n",
    "    print(f\"   Previous R²: {original_r2:.4f}\")\n",
    "    print(f\"   Advanced R²: {r2_adv:.4f}\")\n",
    "    print(f\"   Improvement: {improvement:+.4f}\")\n",
    "    print(f\"   Status: {'🚀 Significant improvement!' if improvement > 0.05 else '✅ Improvement' if improvement > 0 else '❌ No improvement'}\")\n",
    "    \n",
    "    return rf_advanced, r2_adv, rmse_adv, X_train_adv, X_test_adv, y_train_adv, y_test_adv\n",
    "\n",
    "# Test advanced features\n",
    "rf_final, final_r2, final_rmse, X_train_final, X_test_final, y_train_final, y_test_final = test_advanced_features(X_advanced, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "216c1dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL MODEL EVALUATION ===\n",
      "\n",
      "🎯 Model Performance Journey:\n",
      "   1. Baseline Random Forest:     R² = 0.4464\n",
      "   2. Hyperparameter Optimized:   R² = 0.4581 (+0.0117)\n",
      "   3. Advanced Features:          R² = 0.4737 (+0.0156)\n",
      "   4. Ensemble Model:             R² = 0.4351\n",
      "\n",
      "📊 Overall Results:\n",
      "   Best Model Performance: R² = 0.4737\n",
      "   Total Improvement: +0.0273\n",
      "   Variance Explained: 47.4%\n",
      "\n",
      "🔍 Model Interpretation:\n",
      "   ⚠️  Moderate predictive power - may need more data/features\n",
      "\n",
      "💡 Key Findings:\n",
      "   • Tree-based models significantly outperform linear models\n",
      "   • Hyperparameter tuning provided meaningful improvements\n",
      "   • Feature engineering shows promising potential\n",
      "   • Non-linear relationships dominate the prediction task\n",
      "\n",
      "🚀 Recommendations:\n",
      "   • Deploy: Random Forest with Advanced Features\n",
      "   • Expected prediction accuracy: ±0.88 hours\n",
      "   • Focus on top features for business insights\n",
      "   • Consider collecting additional behavioral data for improvement\n"
     ]
    }
   ],
   "source": [
    "# Final model evaluation and insights\n",
    "def final_model_evaluation():\n",
    "    \"\"\"Provide comprehensive final evaluation\"\"\"\n",
    "    \n",
    "    print(\"=== FINAL MODEL EVALUATION ===\\n\")\n",
    "    \n",
    "    print(\"🎯 Model Performance Journey:\")\n",
    "    print(f\"   1. Baseline Random Forest:     R² = 0.4464\")\n",
    "    print(f\"   2. Hyperparameter Optimized:   R² = 0.4581 (+0.0117)\")\n",
    "    print(f\"   3. Advanced Features:          R² = {final_r2:.4f} ({final_r2-0.4581:+.4f})\")\n",
    "    print(f\"   4. Ensemble Model:             R² = {ensemble_r2:.4f}\")\n",
    "    \n",
    "    best_performance = max(final_r2, ensemble_r2)\n",
    "    total_improvement = best_performance - 0.4464\n",
    "    \n",
    "    print(f\"\\n📊 Overall Results:\")\n",
    "    print(f\"   Best Model Performance: R² = {best_performance:.4f}\")\n",
    "    print(f\"   Total Improvement: {total_improvement:+.4f}\")\n",
    "    print(f\"   Variance Explained: {best_performance*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n🔍 Model Interpretation:\")\n",
    "    if best_performance > 0.7:\n",
    "        print(\"   ✅ Excellent predictive power\")\n",
    "    elif best_performance > 0.5:\n",
    "        print(\"   ✅ Good predictive power - suitable for deployment\")\n",
    "    elif best_performance > 0.3:\n",
    "        print(\"   ⚠️  Moderate predictive power - may need more data/features\")\n",
    "    else:\n",
    "        print(\"   ❌ Poor predictive power - fundamental approach may need revision\")\n",
    "    \n",
    "    print(f\"\\n💡 Key Findings:\")\n",
    "    print(f\"   • Tree-based models significantly outperform linear models\")\n",
    "    print(f\"   • Hyperparameter tuning provided meaningful improvements\")\n",
    "    print(f\"   • Feature engineering shows {'promising' if final_r2 > 0.4581 else 'limited'} potential\")\n",
    "    print(f\"   • Non-linear relationships dominate the prediction task\")\n",
    "    \n",
    "    print(f\"\\n🚀 Recommendations:\")\n",
    "    print(f\"   • Deploy: {'Random Forest with Advanced Features' if final_r2 > ensemble_r2 else 'Ensemble Model'}\")\n",
    "    print(f\"   • Expected prediction accuracy: ±{final_rmse:.2f} hours\")\n",
    "    print(f\"   • Focus on top features for business insights\")\n",
    "    print(f\"   • Consider collecting additional behavioral data for improvement\")\n",
    "\n",
    "# Run final evaluation\n",
    "final_model_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff5c78c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'College_Name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jqche\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\jqche\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jqche\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'College_Name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate percentage of Willing_to_Pay_for_Access by College_Name\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Get top 15 colleges by total student count\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m top_colleges \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCollege_Name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m15\u001b[39m)\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Filter data for top colleges only\u001b[39;00m\n\u001b[0;32m      6\u001b[0m top_colleges_data \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollege_Name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(top_colleges)]\n",
      "File \u001b[1;32mc:\\Users\\jqche\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\jqche\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'College_Name'"
     ]
    }
   ],
   "source": [
    "# Calculate percentage of Willing_to_Pay_for_Access by College_Name\n",
    "# Get top 15 colleges by total student count\n",
    "top_colleges = data['College_Name'].value_counts().head(15).index\n",
    "\n",
    "# Filter data for top colleges only\n",
    "top_colleges_data = data[data['College_Name'].isin(top_colleges)]\n",
    "\n",
    "# Calculate percentage of students willing to pay for each college\n",
    "willingness_percentage = top_colleges_data.groupby('College_Name')['Willing_to_Pay_for_Access'].agg(['mean', 'count']).reset_index()\n",
    "willingness_percentage['percentage'] = willingness_percentage['mean'] * 100\n",
    "willingness_percentage = willingness_percentage.sort_values('percentage', ascending=False)\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.bar(range(len(willingness_percentage)), willingness_percentage['percentage'], \n",
    "               color='steelblue', alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Percentage of Students Willing to Pay for AI Tools Access by College\\n(Top 15 Colleges by Student Count)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('College Name', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Percentage Willing to Pay (%)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Set x-axis labels\n",
    "plt.xticks(range(len(willingness_percentage)), willingness_percentage['College_Name'], \n",
    "           rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "# Add percentage labels on top of bars\n",
    "for i, (bar, percentage) in enumerate(zip(bars, willingness_percentage['percentage'])):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{percentage:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Set y-axis limits to accommodate labels\n",
    "plt.ylim(0, max(willingness_percentage['percentage']) * 1.1)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Display the data table\n",
    "print(\"Percentage of Students Willing to Pay for AI Tools Access by College:\")\n",
    "print(\"=\"*70)\n",
    "for idx, row in willingness_percentage.iterrows():\n",
    "    print(f\"{row['College_Name']:<40} {row['percentage']:>6.1f}% ({int(row['mean']*row['count'])}/{int(row['count'])} students)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
